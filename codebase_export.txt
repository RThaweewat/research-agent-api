# research-agent-api Codebase Export

## Project Structure

  ğŸ“„ README.md
  ğŸ“„ codebase_export.txt
  ğŸ“„ docker-compose.yml
  ğŸ“„ postman_test_cases.json
  ğŸ“„ requirements.txt
ğŸ“ tests/
  ğŸ“„ conftest.py
  ğŸ“ unit/
    ğŸ“„ test_graph_service.py
    ğŸ“„ test_llm_service.py
    ğŸ“„ test_memory_service.py
    ğŸ“„ test_retrieval_service.py
  ğŸ“ fixtures/
ğŸ“ docs/
ğŸ“ scripts/
  ğŸ“„ export_codebase.py
ğŸ“ src/
  ğŸ“„ main.py
  ğŸ“ routers/
    ğŸ“„ docs_router.py
    ğŸ“„ query_router.py
  ğŸ“ utils/
    ğŸ“„ config.py
    ğŸ“„ logging.py
    ğŸ“„ pdf_utils.py
  ğŸ“ models/
    ğŸ“„ request_models.py
    ğŸ“„ response_models.py
  ğŸ“ services/
    ğŸ“„ graph_service.py
    ğŸ“„ llm_service.py
    ğŸ“„ memory_service.py
    ğŸ“„ retrieval_service.py
    ğŸ“„ tracing_service.py
  ğŸ“ src/

## File Contents


### ğŸ“„ README.md
```md
# Research Agent API

## Overview
The Research Agent API is an advanced question-answering system built to analyze and retrieve information from research documents. It leverages a combination of cutting-edge LLMs and retrieval techniques, ensuring accurate and efficient responses for research-related queries.

### Features

- **ğŸ¤” Intelligent Question Routing**: Automatically routes questions between the vector store and direct LLM calls.
- **ğŸ” Ensemble Retrieval**: Combines keyword-based retrieval (BM25) and semantic search for robust document retrieval.
- **âœï¸ Dynamic Question Rewriting**: Refines user questions for improved retrieval accuracy.
- **ğŸ® Multi-Stage Answer Generation**: Includes self-evaluation and quality checks for precise answers.
- **ğŸ”€ Adaptive Retrieval Pipeline**: Dynamically rebuilds document indexes for newly uploaded files.
- **ğŸŒ REST API Integration**: Includes endpoints for uploading documents, querying, and resetting threads or vector databases.

### Technology Stack

#### Core Components
- **Language Models**: GPT-4o (via OpenAI API)
- **Vector Database**: FAISS for semantic embeddings
- **Retrieval Techniques**:
  - **Keyword Search**: BM25
  - **Semantic Search**: OpenAI Embeddings
  - **Ensemble Search**: Combining keyword and semantic results
- **Orchestration**: FastAPI for API management
- **Tracing**: Langfuse for tracing and debugging
- **Document Handling**: PyPDF for processing research papers

#### Environment and Dependencies
- **Runtime**: Python 3.12
- **Containerization**: Docker (via `docker-compose`)
- **Key Libraries**:
  - `langgraph`
  - `langchain`
  - `faiss-cpu`
  - `fastapi`
  - `pydantic`
  - `uvicorn`
  - `loguru`

## API Endpoints

### 1. **Upload Research Papers**
**Endpoint**: `POST /api/upload`
- Upload multiple PDFs to populate the knowledge base.
- Returns success and failure details for each file.

### 2. **Query Documents**
**Endpoint**: `POST /api/query`
- Ask a question to retrieve answers from loaded documents or general knowledge.
- Supports conversation continuity using `thread_id`.

### 3. **Reset Thread**
**Endpoint**: `POST /api/thread/reset`
- Clears the memory for a specific thread.

### 4. **Vector DB Reset**
**Endpoint**: `POST /api/vectordb/reset`
- Clears the vector database and resets the retrieval pipeline.

### 5. **Document Status**
**Endpoint**: `GET /api/status`
- Retrieves the status of loaded documents.

## Project Structure

```text
â€¢ docker-compose.yml
â€¢ postman_test_cases.json
â€¢ requirements.txt
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_graph_service.py
â”‚   â”‚   â”œâ”€â”€ test_llm_service.py
â”‚   â”‚   â”œâ”€â”€ test_memory_service.py
â”‚   â”‚   â””â”€â”€ test_retrieval_service.py
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ docs/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ export_codebase.py
â”œâ”€â”€ src/
    â”œâ”€â”€ main.py
    â”œâ”€â”€ routers/
    â”‚   â”œâ”€â”€ docs_router.py
    â”‚   â””â”€â”€ query_router.py
    â”œâ”€â”€ utils/
    â”‚   â”œâ”€â”€ config.py
    â”‚   â”œâ”€â”€ logging.py
    â”‚   â””â”€â”€ pdf_utils.py
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ request_models.py
    â”‚   â””â”€â”€ response_models.py
    â”œâ”€â”€ services/
        â”œâ”€â”€ graph_service.py
        â”œâ”€â”€ llm_service.py
        â”œâ”€â”€ memory_service.py
        â”œâ”€â”€ retrieval_service.py
        â””â”€â”€ tracing_service.py
```

## Installation

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-repo/research-agent-api.git
   cd research-agent-api
   ```

2. **Setup Environment**:
   ```bash
   cp .env.example .env
   # Add your OpenAI and Langfuse keys in the .env file
   ```

3. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the API**:
   ```bash
   uvicorn src.main:app --host 0.0.0.0 --port 8000
   ```

5. **Access the API**:
   Navigate to `http://localhost:8000/docs` for interactive API documentation.

## Testing

Run unit tests using Pytest:
```bash
pytest tests/
```
```


### ğŸ“„ codebase_export.txt
```txt
# research-agent-api Codebase Export

## Project Structure

  ğŸ“„ docker-compose.yml
  ğŸ“„ postman_test_cases.json
  ğŸ“„ requirements.txt
ğŸ“ tests/
  ğŸ“„ conftest.py
  ğŸ“ unit/
    ğŸ“„ test_graph_service.py
    ğŸ“„ test_llm_service.py
    ğŸ“„ test_memory_service.py
    ğŸ“„ test_retrieval_service.py
  ğŸ“ fixtures/
ğŸ“ docs/
ğŸ“ scripts/
  ğŸ“„ export_codebase.py
ğŸ“ src/
  ğŸ“„ main.py
  ğŸ“ routers/
    ğŸ“„ docs_router.py
    ğŸ“„ query_router.py
  ğŸ“ utils/
    ğŸ“„ config.py
    ğŸ“„ logging.py
    ğŸ“„ pdf_utils.py
  ğŸ“ models/
    ğŸ“„ request_models.py
    ğŸ“„ response_models.py
  ğŸ“ services/
    ğŸ“„ graph_service.py
    ğŸ“„ llm_service.py
    ğŸ“„ memory_service.py
    ğŸ“„ retrieval_service.py
    ğŸ“„ tracing_service.py

## File Contents


### ğŸ“„ docker-compose.yml
```yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src
      - ./docs:/app/docs
      - ./.env:/app/.env
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: ["/start.sh"]
```


### ğŸ“„ postman_test_cases.json
```json
{
  "info": {
    "name": "Research Paper Agent API Tests",
    "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
  },
  "item": [
    {
      "name": "1. Initial Chat (Default Thread)",
      "request": {
        "method": "POST",
        "url": {
          "raw": "{{base_url}}/api/query?question=What is Text-to-SQL?",
          "host": ["{{base_url}}"],
          "path": ["api", "query"],
          "query": [
            {
              "key": "question",
              "value": "What is Text-to-SQL?"
            }
          ]
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Response includes thread_id\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.thread_id).to.be.a('string');",
              "    pm.collectionVariables.set(\"thread_id\", jsonData.thread_id);",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "2. Upload Research Papers",
      "request": {
        "method": "POST",
        "header": [],
        "body": {
          "mode": "formdata",
          "formdata": [
            {
              "key": "files",
              "type": "file",
              "src": "./test_files/paper1.pdf"
            },
            {
              "key": "files",
              "type": "file",
              "src": "./test_files/paper2.pdf"
            }
          ]
        },
        "url": "{{base_url}}/api/upload"
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Files uploaded successfully\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.details.successful).to.have.length.above(0);",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "3. Follow-up Question (Same Thread)",
      "request": {
        "method": "POST",
        "url": {
          "raw": "{{base_url}}/api/query?question=What are the main findings about Text-to-SQL in these papers?&thread_id={{thread_id}}",
          "host": ["{{base_url}}"],
          "path": ["api", "query"],
          "query": [
            {
              "key": "question",
              "value": "What are the main findings about Text-to-SQL in these papers?"
            },
            {
              "key": "thread_id",
              "value": "{{thread_id}}"
            }
          ]
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Response shows memory of previous context\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.response).to.not.include('I don\\'t have enough context');",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "4. Reset Thread",
      "request": {
        "method": "POST",
        "url": {
          "raw": "{{base_url}}/api/thread/reset?thread_id={{thread_id}}",
          "host": ["{{base_url}}"],
          "path": ["api", "thread", "reset"],
          "query": [
            {
              "key": "thread_id",
              "value": "{{thread_id}}"
            }
          ]
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Thread reset successful\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.status).to.eql('success');",
              "    pm.expect(jsonData.thread_id).to.eql(pm.collectionVariables.get('thread_id'));",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "5. Test After Thread Reset",
      "request": {
        "method": "POST",
        "url": {
          "raw": "{{base_url}}/api/query",
          "query": [
            {
              "key": "question",
              "value": "What did we discuss about Text-to-SQL earlier?"
            },
            {
              "key": "thread_id",
              "value": "{{thread_id}}"
            }
          ]
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Response shows no memory of previous conversation\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.response).to.include('no previous context');",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "6. Reset Vector Database",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/vectordb/reset"
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "7. Test After VectorDB Reset",
      "request": {
        "method": "POST",
        "url": {
          "raw": "{{base_url}}/api/query",
          "query": [
            {
              "key": "question",
              "value": "What are the findings from the research papers?"
            }
          ]
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Response indicates no documents loaded\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.response).to.include('no documents loaded');",
              "});"
            ]
          }
        }
      ]
    }
  ],
  "variable": [
    {
      "key": "base_url",
      "value": "http://localhost:8000"
    }
  ]
}

```


### ğŸ“„ requirements.txt
```txt
python-dotenv==1.0.1
loguru==0.7.3
fastapi==0.115.6
langchain==0.3.13
langchain-community==0.3.13
pydantic==2.10.4
uvicorn==0.34.0
langchain-core==0.3.28
langchain-openai==0.2.14
langgraph==0.2.60
langfuse==2.57.0
python-multipart==0.0.20
pypdf==5.1.0
faiss-cpu==1.9.0.post1
rank_bm25==0.2.2



```


### ğŸ“„ tests/conftest.py
```py
import os
import pytest
from fastapi.testclient import TestClient
from src.main import app
from src.services.graph_service import GraphService
from src.services.llm_service import LLMService
from src.services.memory_service import MemoryService
from src.services.retrieval_service import RetrievalPipeline
from langchain.schema import Document

@pytest.fixture(scope="session", autouse=True)
def setup_test_env():
    """Setup test environment"""
    fixtures_dir = os.path.join(os.path.dirname(__file__), "fixtures")
    os.makedirs(fixtures_dir, exist_ok=True)

    sample_pdf_path = os.path.join(fixtures_dir, "sample.pdf")
    if not os.path.exists(sample_pdf_path):
        with open(sample_pdf_path, "wb") as f:
            f.write(b"%PDF-1.4\n1 0 obj\n<<\n/Type /Catalog\n>>\nendobj\ntrailer\n<<\n/Root 1 0 R\n>>\n%%EOF")

@pytest.fixture
def test_client():
    return TestClient(app)

@pytest.fixture
def sample_pdf_path():
    return os.path.join(os.path.dirname(__file__), "fixtures", "sample.pdf")

@pytest.fixture
def mock_documents():
    return [
        Document(
            page_content="This is a test document about machine learning.",
            metadata={"source": "test1.pdf"}
        ),
        Document(
            page_content="This document discusses neural networks in detail.",
            metadata={"source": "test2.pdf"}
        )
    ]

@pytest.fixture
def graph_service():
    return GraphService()

@pytest.fixture
def llm_service():
    return LLMService()

@pytest.fixture
def memory_service():
    return MemoryService()

@pytest.fixture
def retrieval_pipeline():
    pipeline = RetrievalPipeline()
    yield pipeline
    pipeline.reset()  # Cleanup after each test 
```


### ğŸ“„ tests/unit/test_graph_service.py
```py
import pytest
from src.services.graph_service import GraphService
from langchain.schema import HumanMessage, AIMessage

def test_process_question_no_documents(graph_service):
    """Test processing question when no documents are loaded"""
    response = graph_service.process_question("What is AI?")
    assert "don't have any research papers loaded" in response.lower()

def test_process_question_with_history(graph_service, mock_documents):
    """Test processing question with conversation history"""
    # First rebuild the retrieval pipeline
    from src.services.retrieval_service import retrieval_pipeline
    retrieval_pipeline.rebuild(mock_documents)
    
    history = [
        HumanMessage(content="What is machine learning?"),
        AIMessage(content="Machine learning is a subset of AI.")
    ]
    response = graph_service._process_with_memory(
        "Can you elaborate on that?",
        history
    )
    assert response is not None
    assert isinstance(response, str)

@pytest.mark.parametrize("recursion_count,expected_contains", [
    (5, "steps"),
    (10, "steps"),
    (15, "steps"),
])
def test_format_error_response(graph_service, recursion_count, expected_contains):
    """Test error response formatting"""
    response = graph_service.format_error_response("Test error", recursion_count)
    assert expected_contains in response.lower()
```


### ğŸ“„ tests/unit/test_llm_service.py
```py
import pytest
from src.services.llm_service import LLMService

def test_llm_initialization():
    """Test LLM service initialization"""
    service = LLMService()
    assert service.model == "gpt-4o-mini"
    assert service.temperature == 0

def test_llm_invoke_with_system_prompt(llm_service):
    """Test LLM invocation with system prompt"""
    response = llm_service.invoke("What is AI?")
    assert isinstance(response, str)
    assert len(response) > 0

@pytest.mark.parametrize("config", [
    {"temperature": 0.7},
    {"max_tokens": 100},
    {"temperature": 0, "max_tokens": 50},
])
def test_llm_invoke_with_config(llm_service, config):
    """Test LLM invocation with different configs"""
    response = llm_service.invoke("Test prompt", config)
    assert isinstance(response, str) 
```


### ğŸ“„ tests/unit/test_memory_service.py
```py
import pytest
from src.services.memory_service import MemoryService
from langchain.schema import HumanMessage, AIMessage

def test_add_and_get_messages(memory_service):
    """Test adding and retrieving messages"""
    thread_id = "test-thread"
    message1 = HumanMessage(content="Test question")
    message2 = AIMessage(content="Test answer")
    
    memory_service.add_message(thread_id, message1)
    memory_service.add_message(thread_id, message2)
    
    messages = memory_service.get_messages(thread_id)
    assert len(messages) == 2
    assert messages[0].content == "Test question"
    assert messages[1].content == "Test answer"

def test_get_last_k_messages(memory_service):
    """Test retrieving last K messages"""
    thread_id = "test-thread"
    for i in range(5):
        memory_service.add_message(
            thread_id,
            HumanMessage(content=f"Message {i}")
        )
    
    messages = memory_service.get_messages(thread_id, last_k=3)
    assert len(messages) == 3
    assert messages[-1].content == "Message 4"

def test_clear_thread(memory_service):
    """Test clearing a conversation thread"""
    thread_id = "test-thread"
    memory_service.add_message(
        thread_id,
        HumanMessage(content="Test")
    )
    
    memory_service.clear_thread(thread_id)
    assert not memory_service.thread_exists(thread_id) 
```


### ğŸ“„ tests/unit/test_retrieval_service.py
```py
import pytest
from src.services.retrieval_service import RetrievalPipeline

def test_pipeline_initialization(retrieval_pipeline):
    """Test retrieval pipeline initialization"""
    assert retrieval_pipeline.vectorstore is None
    assert retrieval_pipeline.documents == []

def test_pipeline_rebuild(retrieval_pipeline, mock_documents):
    """Test rebuilding pipeline with documents"""
    retrieval_pipeline.rebuild(mock_documents)
    assert retrieval_pipeline.has_documents()
    assert len(retrieval_pipeline.documents) == 2

def test_retrieve_with_no_documents(retrieval_pipeline):
    """Test retrieval with no documents loaded"""
    with pytest.raises(ValueError):
        retrieval_pipeline.retrieve("What is AI?")

def test_retrieve_with_documents(retrieval_pipeline, mock_documents):
    """Test document retrieval"""
    retrieval_pipeline.rebuild(mock_documents)
    results = retrieval_pipeline.retrieve("machine learning")
    assert len(results) > 0
    assert "machine learning" in results[0].lower()

@pytest.mark.parametrize("question,expected_count", [
    ("machine learning", 1),
    ("neural networks", 1),
    ("irrelevant query", 0),
])
def test_retrieve_different_queries(
    retrieval_pipeline,
    mock_documents,
    question,
    expected_count
):
    """Test retrieval with different queries"""
    retrieval_pipeline.rebuild(mock_documents)
    results = retrieval_pipeline.retrieve(question)
    assert len([r for r in results if question.lower() in r.lower()]) >= expected_count 
```


### ğŸ“„ scripts/export_codebase.py
```py
import os
from pathlib import Path
import json

def get_file_content(file_path: str) -> str:
    """Read and return file content with proper error handling"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        return f"Error reading file: {str(e)}"

def should_ignore(path: str) -> bool:
    """Check if path should be ignored"""
    ignore_patterns = [
        '__pycache__',
        '.git',
        '.env',
        '.venv',
        'node_modules',
        '.pytest_cache',
        '.coverage',
        '.idea',
        '.vscode',
        'dist',
        'build',
        '*.pyc',
        '*.pyo',
        '*.pyd',
        '.DS_Store',
    ]
    
    return any(pattern in path for pattern in ignore_patterns)

def export_codebase(root_dir: str, output_file: str):
    """Export codebase structure and content to a text file"""
    
    # Get project name from root directory
    project_name = os.path.basename(os.path.abspath(root_dir))
    
    # Initialize output content
    output = [
        f"# {project_name} Codebase Export",
        "\n## Project Structure\n",
    ]
    
    # Track all files for content export
    files_to_export = []
    
    # First, build directory structure
    for root, dirs, files in os.walk(root_dir):
        if should_ignore(root):
            continue
            
        # Calculate relative path and indent level
        rel_path = os.path.relpath(root, root_dir)
        indent = '  ' * (len(Path(rel_path).parts) - 1)
        
        # Add directory to structure
        if rel_path != '.':
            output.append(f"{indent}ğŸ“ {os.path.basename(root)}/")
        
        # Add files to structure and track for content export
        for file in sorted(files):
            if file.endswith(('.py', '.txt', '.json', '.yml', '.yaml', '.md', '.env.example')):
                file_path = os.path.join(root, file)
                rel_file_path = os.path.relpath(file_path, root_dir)
                output.append(f"{indent}  ğŸ“„ {file}")
                files_to_export.append(rel_file_path)
    
    # Add file contents
    output.append("\n## File Contents\n")
    
    for file_path in files_to_export:
        abs_path = os.path.join(root_dir, file_path)
        output.extend([
            f"\n### ğŸ“„ {file_path}",
            "```" + (file_path.split('.')[-1] if '.' in file_path else ''),
            get_file_content(abs_path),
            "```\n"
        ])
    
    # Write to output file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(output))

if __name__ == "__main__":
    # Get the project root directory (assuming this script is in a scripts folder)
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_path = os.path.join(project_root, "codebase_export.txt")
    
    export_codebase(project_root, output_path)
    print(f"Codebase exported to: {output_path}") 
```


### ğŸ“„ src/main.py
```py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.routers import query_router, docs_router
from src.utils.config import DOCS_FOLDER
from src.services.retrieval_service import retrieval_pipeline
from src.utils.pdf_utils import load_pdfs_from_directory

# Pre-load documents if any exist at startup
initial_docs = load_pdfs_from_directory(DOCS_FOLDER)
if initial_docs:
    retrieval_pipeline.rebuild(initial_docs)

app = FastAPI(
    title="Osram Product Q&A API",
    description="An API for uploading PDFs and querying information about Osram products."
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(query_router.router)
app.include_router(docs_router.router)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

```


### ğŸ“„ src/routers/docs_router.py
```py
from fastapi import APIRouter, UploadFile, File, HTTPException
from typing import List
from src.services.retrieval_service import retrieval_pipeline
from src.utils.pdf_utils import process_uploaded_pdf
from loguru import logger

router = APIRouter(prefix="/api")

@router.post("/upload")
async def upload_documents(files: List[UploadFile] = File(...)):
    """Upload multiple PDF documents to be processed and added to the knowledge base"""
    try:
        if not files:
            raise HTTPException(status_code=400, detail="No files provided")

        results = {
            "successful": [],
            "failed": []
        }
        
        all_docs = []

        for file in files:
            try:
                if not file.filename.endswith('.pdf'):
                    results["failed"].append({
                        "filename": file.filename,
                        "error": "Only PDF files are supported"
                    })
                    continue

                docs = await process_uploaded_pdf(file)
                all_docs.extend(docs)
                
                results["successful"].append({
                    "filename": file.filename,
                    "pages": len(docs)
                })
                
            except Exception as e:
                logger.error(f"Error processing file {file.filename}: {e}")
                results["failed"].append({
                    "filename": file.filename,
                    "error": str(e)
                })

        if all_docs:
            retrieval_pipeline.rebuild(all_docs)
            logger.info(f"Rebuilt retrieval pipeline with {len(all_docs)} documents")

        response = {
            "message": f"Processed {len(results['successful'])} files successfully, {len(results['failed'])} failed",
            "details": results
        }
        
        # If all files failed, return 400
        if not results["successful"] and results["failed"]:
            raise HTTPException(status_code=400, detail=response)
            
        return response
        
    except Exception as e:
        logger.exception(f"Error in upload_documents: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/status")
async def get_document_status():
    """Get the status of loaded documents"""
    try:
        has_docs = retrieval_pipeline.has_documents()
        return {
            "has_documents": has_docs,
            "document_count": len(retrieval_pipeline.documents) if has_docs else 0
        }
    except Exception as e:
        logger.exception(f"Error getting document status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

```


### ğŸ“„ src/routers/query_router.py
```py
from fastapi import APIRouter, Query, HTTPException
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field
from typing import Literal, Optional, List
from src.services.memory_service import memory_service
from src.services.llm_service import llm_service
from src.services.graph_service import graph_service
from src.services.retrieval_service import RetrievalPipeline, retrieval_pipeline
import uuid
from loguru import logger
from langchain.schema import HumanMessage, AIMessage
from enum import Enum
from src.services.tracing_service import tracing_service

router = APIRouter(prefix="/api")

class QueryType(str, Enum):
    MEMORY = "memory"
    VECTORSTORE = "vectorstore"
    GENERAL = "general"

class RouteQuery(BaseModel):
    """Route a user query to the most appropriate processing method."""
    query_type: Literal["memory", "vectorstore", "general"] = Field(
        ...,
        description="Type of query processing needed"
    )
    reason: str = Field(
        ...,
        description="Explanation for routing decision"
    )

class MemoryResponse(BaseModel):
    """Structure the response for memory-related queries."""
    response_type: Literal["count", "list", "history"] = Field(
        ...,
        description="Type of memory response"
    )
    content: str = Field(
        ...,
        description="The formatted response content"
    )

# Setup LLM and Chains
llm = ChatOpenAI(
    model="gpt-4o-mini", 
    temperature=0,
    callbacks=[tracing_service.get_handler()]
)
structured_router = llm.with_structured_output(RouteQuery)
memory_chain = llm.with_structured_output(MemoryResponse)

# Router Prompt
router_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an expert at routing user questions to appropriate handlers.
    Route to:
    - 'memory' for questions about conversation history, previous questions, or counts
    - 'vectorstore' for questions requiring research paper knowledge
    - 'general' for basic questions not requiring special context
    
    Analyze the question carefully and provide a clear reason for your choice."""),
    ("human", "{question}")
])

memory_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an expert at analyzing conversation history questions.
    Determine the appropriate response type:
    - 'count' for questions about number of interactions
    - 'list' for requests to see previous questions
    - 'history' for general conversation history requests
    
    Format the response appropriately based on the type."""),
    ("human", """Question: {question}
    Conversation History: {history}
    
    Determine response type and format appropriate response.""")
])

router_chain = router_prompt | structured_router
memory_chain = memory_prompt | memory_chain

def handle_memory_question(question: str, thread_id: str, tags: Optional[List[str]] = None) -> dict:
    """Handle questions about conversation history"""
    messages = memory_service.get_messages(thread_id)
    history = "\n".join([f"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}" 
                        for m in messages])

    memory_result = memory_chain.invoke(
        {
            "question": question,
            "history": history
        },
        config={
            "callbacks": [tracing_service.get_handler()],
            "tags": tags or ["memory", "history", "conversation"]
        }
    )

    memory_service.add_message(thread_id, HumanMessage(content=question))
    memory_service.add_message(thread_id, AIMessage(content=memory_result.content))
    
    return {
        "response": memory_result.content,
        "thread_id": thread_id,
        "source": "memory",
        "response_type": memory_result.response_type
    }

def handle_general_question(question: str, thread_id: str, tags: Optional[List[str]] = None) -> dict:
    """Handle general questions using LLM"""
    response = llm_service.invoke(
        question,
        config={
            "callbacks": [tracing_service.get_handler()],
            "tags": tags or ["general", "llm", "direct"]
        }
    )
    memory_service.add_message(thread_id, HumanMessage(content=question))
    memory_service.add_message(thread_id, AIMessage(content=response))
    
    return {
        "response": response,
        "thread_id": thread_id,
        "source": "general"
    }

@router.post("/query")
async def query_documents(
    question: str = Query(..., description="Question to ask"),
    thread_id: str = Query(None, description="Thread ID for conversation continuity")
):
    """Route and process questions based on their type"""
    try:
        if not thread_id:
            thread_id = str(uuid.uuid4())

        with tracing_service.trace_interaction(
            "query",
            metadata={
                "question": question,
                "thread_id": thread_id
            },
            tags=["query", "interaction"]
        ) as trace:
            route = router_chain.invoke(
                {"question": question},
                config={"callbacks": [tracing_service.get_handler()]}
            )
            
            if trace:
                trace.score(
                    name="routing_decision",
                    value=1.0,
                    comment=f"Routed to: {route.query_type}"
                )
                trace.tags.append(route.query_type)

            if route.query_type == "memory":
                messages = memory_service.get_messages(thread_id)
                tracing_service.log_memory_access(
                    thread_id, 
                    len(messages),
                    tags=["memory", "history", "conversation"]
                )
                response = handle_memory_question(question, thread_id)
                
            elif route.query_type == "vectorstore":
                handler = tracing_service.get_handler()
                response = graph_service.process_question(
                    question, 
                    thread_id,
                    config={
                        "callbacks": [handler],
                        "tags": ["vectorstore", "retrieval", "research"]
                    }
                )
                
            else:  # general
                response = handle_general_question(
                    question, 
                    thread_id,
                    tags=["general", "llm", "direct"]
                )
                
            if trace:
                trace.score(
                    name="response_generated",
                    value=1.0,
                    comment="Successfully generated response"
                )
                
            return response
            
    except Exception as e:
        logger.exception("Error processing query")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/thread/reset")
async def reset_thread(thread_id: str = Query(..., description="Thread ID to reset")):
    """Reset a conversation thread"""
    try:
        if not memory_service.thread_exists(thread_id):
            raise HTTPException(
                status_code=404, 
                detail=f"Thread {thread_id} not found"
            )
            
        memory_service.clear_thread(thread_id)
        logger.info(f"Thread {thread_id} reset successfully")
        return {
            "status": "success",
            "message": "Thread reset successfully",
            "thread_id": thread_id
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Error in reset_thread")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/vectordb/reset")
async def reset_vectordb():
    """Reset the vector database"""
    try:
        # Reset the existing pipeline instead of creating new one
        retrieval_pipeline.reset()
        
        logger.info("Vector database reset successfully")
        return {
            "status": "success",
            "message": "Vector database reset successfully",
            "document_count": 0,
            "has_documents": False
        }
    except Exception as e:
        logger.exception("Error in reset_vectordb")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/thread/status/{thread_id}")
async def get_thread_status(thread_id: str):
    """Get the status of a conversation thread"""
    try:
        messages = memory_service.get_messages(thread_id)
        return {
            "thread_id": thread_id,
            "message_count": len(messages),
            "has_history": len(messages) > 0,
            "messages": [
                {
                    "type": "human" if isinstance(msg, HumanMessage) else "ai",
                    "content": msg.content
                }
                for msg in messages
            ]
        }
    except Exception as e:
        logger.exception("Error getting thread status")
        raise HTTPException(status_code=500, detail=str(e))

```


### ğŸ“„ src/utils/config.py
```py
import os
from dotenv import load_dotenv

load_dotenv()

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Langfuse
LANGFUSE_PUBLIC_KEY = os.getenv("LANGFUSE_PUBLIC_KEY")
LANGFUSE_SECRET_KEY = os.getenv("LANGFUSE_SECRET_KEY")
LANGFUSE_HOST = os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")

# Docs
DOCS_FOLDER = "src/docs"

DEFAULT_USER_ID = "default_user"
DEFAULT_THREAD_ID = "default_thread"
LLM_MODEL = "gpt-4o-mini"

```


### ğŸ“„ src/utils/logging.py
```py
from loguru import logger
import sys

# Configure loguru
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO"
)

# Add file logging
logger.add(
    "logs/app.log",
    rotation="500 MB",
    retention="10 days",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    level="DEBUG"
)

```


### ğŸ“„ src/utils/pdf_utils.py
```py
import os
from typing import List
from fastapi import UploadFile
from langchain.docstore.document import Document
from langchain_community.document_loaders import PyPDFLoader
from src.utils.config import DOCS_FOLDER

async def process_uploaded_pdf(file: UploadFile) -> List[Document]:
    """
    Process an uploaded PDF file and return the extracted documents
    
    Args:
        file (UploadFile): The uploaded PDF file
        
    Returns:
        List[Document]: List of extracted documents from the PDF
    """
    # Create the docs directory if it doesn't exist
    os.makedirs(DOCS_FOLDER, exist_ok=True)
    
    # Save the uploaded file
    file_path = os.path.join(DOCS_FOLDER, file.filename)
    content = await file.read()
    
    with open(file_path, "wb") as f:
        f.write(content)
    
    try:
        # Load and process the PDF
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        
        # Concatenate pages belonging to the same document
        if documents:
            combined_doc = documents[0]
            for doc in documents[1:]:
                combined_doc.page_content += "\n\n" + doc.page_content
            return [combined_doc]
        
        return []
        
    except Exception as e:
        # Clean up the file if processing fails
        if os.path.exists(file_path):
            os.remove(file_path)
        raise Exception(f"Error processing PDF: {str(e)}")

def load_pdfs_from_directory(directory: str) -> List[Document]:
    """
    Load all PDFs from a directory
    
    Args:
        directory (str): Path to directory containing PDFs
        
    Returns:
        List[Document]: List of documents from all PDFs
    """
    if not os.path.exists(directory):
        return []
        
    documents = []
    for filename in os.listdir(directory):
        if filename.endswith('.pdf'):
            file_path = os.path.join(directory, filename)
            try:
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                
                # Combine pages of the same document
                if docs:
                    combined_doc = docs[0]
                    for doc in docs[1:]:
                        combined_doc.page_content += "\n\n" + doc.page_content
                    documents.append(combined_doc)
                    
            except Exception as e:
                print(f"Error loading {filename}: {str(e)}")
                continue
                
    return documents

def load_pdf(file_path: str) -> List[Document]:
    loader = PyPDFLoader(file_path)
    return loader.load()

```


### ğŸ“„ src/models/request_models.py
```py
from pydantic import BaseModel, Field
from typing import Optional

class QueryRequest(BaseModel):
    question: str
    user_id: Optional[str] = None
    thread_id: Optional[str] = None

class UploadPDFRequest(BaseModel):
    # TODO: Might be better to separate to pdf_util.py
    pass

```


### ğŸ“„ src/models/response_models.py
```py
from pydantic import BaseModel

class QueryResponse(BaseModel):
    response: str

```


### ğŸ“„ src/services/graph_service.py
```py
from typing import Dict, List, TypedDict, Optional
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langgraph.graph import END, START, StateGraph
from loguru import logger
from src.services.llm_service import llm_service
from langgraph import errors as langgraph_errors
from langchain.schema import HumanMessage, AIMessage, BaseMessage
from src.services.memory_service import memory_service
from src.services.tracing_service import tracing_service
import uuid

class GraphState(TypedDict):
    question: str
    generation: str
    documents: List[str]
    retries: int

class GraphService:
    def __init__(self, base_recursion_limit=15):
        logger.info(f"Initializing GraphService with recursion limit: {base_recursion_limit}")
        self.base_recursion_limit = base_recursion_limit
        self.recursion_count = 0
        self.setup_prompts()
        self.setup_chains()
        self.setup_graph()

    def setup_prompts(self):
        # Question Router
        self.question_router_prompt = PromptTemplate(
            template="""You are an expert research paper analyst.
            For questions about specific content, findings, or details from the research papers, use 'vectorstore'.
            For general research concepts or methodology questions, use 'normal_llm'.
            
            Return a JSON with a single key 'datasource' and no preamble.
            Question to analyze: '''{question}'''""",
            input_variables=["question"]
        )

        # Question Rewriter
        self.question_rewriter_prompt = PromptTemplate(
            template="""You are an expert at analyzing research papers and improving search queries.
            Convert the input question to a better version that will help find relevant information from academic papers.
            
            Original Question: {question}
            
            Rules:
            1. Focus on key research terms and concepts
            2. Include relevant academic terminology
            3. Consider methodology, findings, and conclusions
            4. Add context about research domains when relevant
            5. Keep academic precision while remaining clear
            
            Improved question:""",
            input_variables=["question"]
        )

        # RAG Generation
        self.rag_prompt = PromptTemplate(
            template="""You are a research paper analysis assistant tasked with answering questions about academic content.
            Analyze the following context from research papers to answer the question accurately and concisely.
            
            {context}

            Question: {question}

            Guidelines:
            1. Keep responses clear and concise (max 3-4 sentences per point)
            2. Lead with key numerical findings when available
            3. Focus on concrete research findings and methodology
            4. Cite specific papers when presenting key findings
            5. If information is missing, state it briefly
            6. Use bullet points for multiple findings
            7. Reference previous conversation context when relevant
            8. Maintain continuity with previous responses
            
            Provide a focused, data-driven answer based on the research context.
            
            Answer:""",
            input_variables=["question", "context"]
        )

        # Document Grader
        self.retrieval_grader_prompt = PromptTemplate(
            template="""Evaluate if this research paper excerpt contains information relevant to answering the question.
            
            Research Question: {question}
            
            Document Excerpt:
            -----------
            {document}
            -----------
            
            Evaluation Criteria:
            1. Contains relevant research findings
            2. Discusses related methodology
            3. Presents relevant data or results
            4. Provides theoretical background
            5. Offers relevant conclusions
            
            Respond with a JSON object containing a 'score' key with value 'yes' or 'no'.
            If the document contains ANY academically relevant information that could help answer the question, respond with 'yes'.
            
            Response:""",
            input_variables=["question", "document"]
        )

        # Hallucination Grader
        self.hallucination_grader_chain = PromptTemplate(
            template="""You are evaluating the factual accuracy of research paper analysis.
            
            Research Paper Excerpts:
            ----------
            {documents}
            ----------
            
            Generated Analysis: {generation}
            
            Evaluate whether the analysis is fully supported by the research papers.
            Check for:
            1. Accurate representation of findings
            2. Correct citation of methodology
            3. Proper interpretation of results
            4. Supported conclusions
            5. Appropriate academic context
            
            Return JSON with key 'score': 'yes' if fully supported, 'no' if contains unsupported claims.""",
            input_variables=["documents", "generation"]
        ) | llm_service.llm | JsonOutputParser()

    def setup_chains(self):
        self.question_router = self.question_router_prompt | llm_service.llm | JsonOutputParser()
        self.question_rewriter = self.question_rewriter_prompt | llm_service.llm | StrOutputParser()
        self.rag_chain = self.rag_prompt | llm_service.llm | StrOutputParser()
        self.retrieval_grader = self.retrieval_grader_prompt | llm_service.llm | JsonOutputParser()
        self.hallucination_grader_chain = PromptTemplate(
            template="""You are a grader assessing whether an answer is grounded in facts of the document.
            Here are the documents:
            ----------
            {documents}
            ----------
            Here is the answer: {generation}
            
            Is the generated answer fully supported by the documents? 
            Return JSON with key 'score': 'yes' or 'no'""",
            input_variables=["documents", "generation"]
        ) | llm_service.llm | JsonOutputParser()

    def route_question(self, state: Dict) -> str:
        """Route question to vectorstore or normal LLM"""
        logger.info("Routing question...")
        source = self.question_router.invoke({"question": state["question"]})
        logger.info(f"Routing decision: {source}")
        return "vectorstore"  

    def retrieve_documents(self, state: Dict) -> Dict:
        """Retrieve relevant documents"""
        from src.services.retrieval_service import retrieval_pipeline
        logger.info("Retrieving documents...")
        docs = retrieval_pipeline.retrieve(state["question"])
        logger.info(f"Retrieved {len(docs)} documents")
        return {"documents": docs, "question": state["question"]}

    def grade_documents(self, state: Dict) -> Dict:
        """Grade document relevance"""
        logger.info("Grading documents...")
        question = state["question"]
        documents = state["documents"]
        logger.info(f"Grading {len(documents)} documents for relevance")
        
        filtered_docs = []
        for doc in documents:
            try:
                grading_result = self.retrieval_grader.invoke({
                    "question": question, 
                    "document": doc
                })

                if isinstance(grading_result, dict):
                    is_relevant = grading_result.get("score") == "yes"
                else:
                    # If not JSON, check for positive indicators in the response
                    response_text = str(grading_result).lower()
                    is_relevant = any(word in response_text 
                                    for word in ["yes", "relevant", "related", "contains"])
                
                if is_relevant:
                    filtered_docs.append(doc)
                    logger.info(f"Document marked as relevant")
                else:
                    logger.debug(f"Document marked as not relevant")
                    
            except Exception as e:
                logger.warning(f"Error grading document: {e}")
                filtered_docs.append(doc)
        
        logger.info(f"Filtered from {len(documents)} to {len(filtered_docs)} relevant documents")
        return {"documents": filtered_docs, "question": question}

    def transform_query(self, state: Dict) -> Dict:
        """Improve the query"""
        logger.info("Starting query transformation...")
        original_question = state["question"]
        
        try:
            better_question = self.question_rewriter.invoke({
                "question": original_question
            })
            logger.info(f"Query transformed: '{original_question}' -> '{better_question}'")
            
            return {
                "documents": state["documents"],
                "question": better_question,
                "original_question": original_question
            }
        except Exception as e:
            logger.error(f"Error transforming query: {e}")
            return state

    def generate_answer(self, state: Dict) -> Dict:
        """Generate final answer"""
        logger.info("Generating answer...")
        if not state.get("documents"):
            logger.warning("No documents available for answer generation")
            return {
                "generation": "I couldn't find relevant information in the documents."
            }
            
        try:
            context = "\n".join(state["documents"])
            logger.info(f"Generating answer with context length: {len(context)}")
            
            generation = self.rag_chain.invoke({
                "context": context, 
                "question": state["question"]
            })
            
            logger.info(f"Generated answer: {generation}")
            return {
                "generation": generation
            }
            
        except Exception as e:
            logger.error(f"Error in generate_answer: {e}", exc_info=True)
            return {
                "generation": "An error occurred while generating the answer."
            }

    def setup_graph(self):
        workflow = StateGraph(GraphState)

        workflow.add_node("normal_llm", self.normal_llm)
        workflow.add_node("retrieve", self.retrieve_documents)
        workflow.add_node("grade_documents", self.grade_documents)
        workflow.add_node("generate", self.generate_answer)
        workflow.add_node("transform_query", self.transform_query)

        workflow.add_conditional_edges(
            START,
            self.route_question,
            {
                "normal_llm": "normal_llm",
                "vectorstore": "retrieve"
            }
        )
        workflow.add_edge("normal_llm", END)
        workflow.add_edge("retrieve", "grade_documents")
        workflow.add_conditional_edges(
            "grade_documents",
            self.decide_to_generate,
            {
                "transform_query": "transform_query",
                "generate": "generate"
            }
        )
        workflow.add_edge("transform_query", "retrieve")
        workflow.add_conditional_edges(
            "generate",
            self.grade_generation,
            {
                "not_supported": "transform_query",
                "useful": END
            }
        )

        self.app = workflow.compile()

    def normal_llm(self, state: Dict) -> Dict:
        """Direct LLM response for general research questions"""
        logger.info("Using normal LLM for response")
        question = state["question"]
        try:
            generation = llm_service.invoke(
                f"""Answer this research-related question based on general academic knowledge.
                If the question requires specific paper findings, indicate that document analysis would be needed.
                
                Question: {question}
                """
            )
            return {"generation": str(generation)}
        except Exception as e:
            logger.error(f"Error in normal LLM: {e}")
            return {"generation": "I encountered an error processing your academic question."}

    def decide_to_generate(self, state: Dict) -> str:
        """Decide whether to generate answer or rephrase query"""
        logger.info("ğŸ¤” Deciding next step...")
        documents = state.get("documents", [])
        
        if not documents or len(documents) == 0:
            logger.info("No relevant documents found - will try query rewriting")
            return "transform_query"

        if state.get("original_question"):
            logger.info("Already attempted query rewriting - proceeding to generate")
            return "generate"

        relevance_score = len(documents)
        if relevance_score < 1:
            logger.info(f"Low relevance score ({relevance_score}) - will try query rewriting")
            return "transform_query"
        
        logger.info(f"Found {len(documents)} relevant documents - proceeding to generate")
        return "generate"

    def grade_generation(self, state: Dict) -> str:
        """Grade the generated answer for quality"""
        logger.info("Grading generated answer")
        try:
            # Use the chain instead of calling invoke directly
            grading_result = self.hallucination_grader_chain.invoke({
                "documents": "\n".join(state["documents"]),
                "generation": state["generation"]
            })
            
            if grading_result.get("score") == "yes":
                logger.info("Answer passed quality check")
                return "useful"
            
            logger.info("Answer needs improvement")
            return "not_supported"
            
        except Exception as e:
            logger.error(f"Error in grading: {e}", exc_info=True)
            return "not_supported"

    def hallucination_grader(self, question: str, documents: List[str], generation: str) -> Dict:
        """Grade whether the answer is grounded in the documents"""
        try:
            doc_text = "\n".join(documents)
            prompt = f"""
            Question: {question}
            Documents: {doc_text}
            Generated Answer: {generation}
            
            Is the generated answer fully supported by the documents? 
            Return JSON: {{"score": "yes"}} or {{"score": "no"}}
            """
            
            response = llm_service.invoke(prompt)
            return {"score": "yes" if "yes" in response.lower() else "no"}
            
        except Exception as e:
            logger.error(f"Error in hallucination grading: {e}")
            return {"score": "no"}

    def calculate_recursion_limit(self, documents: List[str]) -> int:
        """Calculate recursion limit based on document complexity"""
        if not documents:
            return self.base_recursion_limit

        doc_count = len(documents)
        avg_length = sum(len(doc) for doc in documents) / doc_count

        limit = min(
            self.base_recursion_limit + (doc_count * 2) + int(avg_length / 1000),
            100  # Hard maximum
        )
        
        logger.info(f"Calculated recursion limit: {limit} (docs: {doc_count}, avg_length: {int(avg_length)})")
        return int(limit)

    def process_question(self, question: str, thread_id: str = None, config: Optional[Dict] = None) -> str:
        """Process a question with memory"""
        try:
            if not thread_id:
                thread_id = str(uuid.uuid4())
                
            # Start a trace for this question
            with tracing_service.trace_interaction(
                "query",
                metadata={
                    "question": question,
                    "thread_id": thread_id
                }
            ) as trace:
                from src.services.retrieval_service import retrieval_pipeline
                if not retrieval_pipeline.has_documents():
                    response = """I don't have any research papers loaded in my database yet. 

To help you effectively, please:
1. Upload relevant research papers using the /upload endpoint
2. Make sure the papers are in PDF format
3. Once uploaded, I'll be able to analyze them and answer your questions

You can also:
- Check the current document status using /api/status
- Upload multiple papers at once
- Reset the database if needed using /api/vectordb/reset

Would you like me to help you with the upload process?"""

                    memory_service.add_message(thread_id, HumanMessage(content=question))
                    memory_service.add_message(thread_id, AIMessage(content=response))
                    
                    return response

                messages = memory_service.get_messages(thread_id, last_k=5)
                current_message = HumanMessage(content=question)
                memory_service.add_message(thread_id, current_message)

                result = self._process_with_memory(question, messages, config)
                
                if result:
                    ai_message = AIMessage(content=result)
                    memory_service.add_message(thread_id, ai_message)
                    
                return result

        except Exception as e:
            logger.exception("Error processing question")
            return self.format_error_response(str(e), self.recursion_count)

    def _log_attempt(self, output: Dict) -> None:
        """Log attempt details"""
        if "transform_query" in output:
            logger.info(f"Query transformation: {output['transform_query'].get('original_question')} -> {output['transform_query'].get('question')}")
        elif "generate" in output:
            logger.info(f"Generation attempt: {output['generate'].get('generation', '')[:100]}...")

    def _create_attempt_record(self, output: Dict) -> Dict:
        """Create a record of an attempt"""
        return {
            "step": self.recursion_count,
            "action": "query_rewrite" if "transform_query" in output else "generate",
            "original": output.get("transform_query", {}).get("original_question"),
            "rewritten": output.get("transform_query", {}).get("question"),
            "result": output.get("generate", {}).get("generation")
        }

    def format_recursion_limit_response(self, attempts) -> str:
        """Format a concise response when recursion limit is reached"""
        response = [
            f"After {self.recursion_count} attempts, I couldn't find a complete answer.",
            "\nLast 3 attempts:"
        ]

        for attempt in attempts[-3:]:
            if attempt["action"] == "query_rewrite":
                response.append(f"\nAttempt {attempt['step']}: Rewriting")
                response.append(f"  Original: {attempt['original'][:100]}...")
                response.append(f"  Rewritten: {attempt['rewritten'][:100]}...")
            elif attempt["action"] == "generate":
                response.append(f"\nAttempt {attempt['step']}: Generating")
                response.append(f"  Result: {attempt['result'][:150]}...")

        response.append("\nSuggestions:")
        response.append("1. Be more specific")
        response.append("2. Try different keywords")
        response.append("3. Add more relevant documents")

        return "\n".join(response)

    def format_cycle_detected_response(self, attempts: List[Dict], cycle_count: int) -> str:
        """Format response when a processing cycle is detected"""
        response = [
            f"I detected a repetitive pattern after {self.recursion_count} steps (cycle repeated {cycle_count} times).",
            "This usually means I'm stuck between similar answers.",
            "\nHere's what I tried:"
        ]

        for attempt in attempts[-3:]:
            if attempt["action"] == "query_rewrite":
                response.append(f"\nStep {attempt['step']}: Rewrote question")
                response.append(f"  From: {attempt['original']}")
                response.append(f"  To: {attempt['rewritten']}")
            elif attempt["action"] == "generate":
                response.append(f"\nStep {attempt['step']}: Attempted answer")
                response.append(f"  Result: {attempt['result']}")

        response.append("\nTo get better results, try:")
        response.append("1. Being more specific in your question")
        response.append("2. Asking about a different aspect")
        response.append("3. Breaking your question into smaller parts")

        return "\n".join(response)

    def format_error_response(self, error: str, steps: int) -> str:
        """Format a helpful error response"""
        return f"""
        I encountered an issue while processing your question after {steps} steps.
        
        Technical details: {error}
        
        To resolve this, you could:
        1. Try asking your question in a different way
        2. Break your question into smaller parts
        3. Provide additional context or documents
        4. Check if the uploaded documents contain the information you're looking for
        """

    def _process_with_memory(self, question: str, history: List[BaseMessage], config: Optional[Dict] = None) -> str:
        """Process a question with conversation history"""
        try:
            self.recursion_count = 0
            result = None
            attempts = []
            
            # Get documents
            docs = self.retrieve_documents({"question": question}).get("documents", [])
            recursion_limit = min(self.base_recursion_limit + len(docs), 15)
            
            logger.info(f"Processing question with limit {recursion_limit}: {question}")

            context = ""
            if history:
                context = "Previous conversation:\n"
                for msg in history[-3:]:
                    if isinstance(msg, HumanMessage):
                        context += f"Human: {msg.content}\n"
                    else:
                        context += f"Assistant: {msg.content}\n"
                context += "\nCurrent question: " + question
            else:
                context = question

            try:
                stream_config = {"recursion_limit": recursion_limit}
                if config:
                    stream_config.update(config)
                    
                for output in self.app.stream(
                    {"question": context},
                    stream_config
                ):
                    self.recursion_count += 1
                    logger.debug(f"Step {self.recursion_count}/{recursion_limit}")

                    if "transform_query" in output or "generate" in output:
                        self._log_attempt(output)
                        attempts.append(self._create_attempt_record(output))

                    if "generate" in output and output["generate"].get("generation"):
                        result = output["generate"]["generation"]
                        logger.success(f"Found answer after {self.recursion_count} steps")
                        break

                return result if result else self.format_recursion_limit_response(attempts)

            except langgraph_errors.GraphRecursionError as e:
                logger.warning(f"Recursion limit reached: {e}")
                return self.format_error_response(
                    "Maximum number of attempts reached without finding a complete answer. The documents may not contain the information needed.",
                    self.recursion_count
                )

        except Exception as e:
            logger.exception("Error in _process_with_memory")
            return self.format_error_response(str(e), self.recursion_count)

graph_service = GraphService()

```


### ğŸ“„ src/services/llm_service.py
```py
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage
from src.utils.config import OPENAI_API_KEY
from src.services.tracing_service import tracing_service
from loguru import logger
from typing import Optional, Dict

class LLMService:
    def __init__(self, model="gpt-4o-mini", temperature=0):
        self.model = model
        self.temperature = temperature
        self._llm = None
        self.system_prompt = """You are a Research Paper Agent, build for rereasrch paper chatbot
Always maintain a professional, academic tone while being helpful and clear."""
        
        self.setup_llm()
    
    def setup_llm(self):
        """Initialize the LLM with error handling"""
        try:
            self._llm = ChatOpenAI(
                model=self.model,
                temperature=self.temperature,
                api_key=OPENAI_API_KEY,
                callbacks=[tracing_service.get_handler()]
            )
            logger.info(f"Initialized LLM with model: {self.model}")
        except Exception as e:
            logger.error(f"Error initializing LLM: {e}")
            raise
    
    @property
    def llm(self):
        """Get LLM instance with lazy loading"""
        if not self._llm:
            self.setup_llm()
        return self._llm
    
    def invoke(self, prompt: str, config: Optional[Dict] = None) -> str:
        """Invoke LLM with error handling"""
        try:
            logger.debug(f"Invoking LLM with prompt: {prompt[:100]}...")

            messages = [
                SystemMessage(content=self.system_prompt),
                {"role": "user", "content": prompt}
            ]
            
            # Merge configs
            invoke_config = {}
            if config:
                invoke_config.update(config)
            
            response = self.llm.invoke(
                messages,
                config=invoke_config
            )
            response_text = str(response.content) if hasattr(response, 'content') else str(response)
            logger.debug(f"LLM response: {response_text[:100]}...")
            return response_text
        except Exception as e:
            logger.exception(f"Error invoking LLM: {e}")
            raise

llm_service = LLMService() 
```


### ğŸ“„ src/services/memory_service.py
```py
from typing import Dict, List
from langchain_core.messages import BaseMessage
from loguru import logger

class MemoryService:
    def __init__(self):
        self.conversations: Dict[str, List[BaseMessage]] = {}
        
    def add_message(self, thread_id: str, message: BaseMessage):
        """Add a message to a conversation thread"""
        if thread_id not in self.conversations:
            self.conversations[thread_id] = []
        self.conversations[thread_id].append(message)
        logger.debug(f"Added message to thread {thread_id}. Total messages: {len(self.conversations[thread_id])}")
        
    def get_messages(self, thread_id: str, last_k: int = None) -> List[BaseMessage]:
        """Get messages from a conversation thread"""
        messages = self.conversations.get(thread_id, [])
        logger.debug(f"Retrieved {len(messages)} messages from thread {thread_id}")
        if last_k:
            messages = messages[-last_k:]
        return messages
        
    def clear_thread(self, thread_id: str):
        """Clear a conversation thread"""
        if thread_id in self.conversations:
            del self.conversations[thread_id]
            logger.info(f"Cleared thread {thread_id}")
            
    def thread_exists(self, thread_id: str) -> bool:
        """Check if a thread exists"""
        return thread_id in self.conversations

memory_service = MemoryService()

```


### ğŸ“„ src/services/retrieval_service.py
```py
import os
import logging
from typing import List
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers import EnsembleRetriever

from src.utils.config import OPENAI_API_KEY

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]  # Try to split on paragraph breaks first
)

class RetrievalPipeline:
    def __init__(self):
        self.vectorstore = None
        self.keyword_retriever = None
        self.ensemble_retriever = None
        self.documents = []
        self.embeddings = None
        
    def reset(self):
        """Completely reset the pipeline"""
        self.vectorstore = None
        self.keyword_retriever = None
        self.ensemble_retriever = None
        self.documents = []
        self.embeddings = None
        logger.info("Pipeline completely reset")
        
    def rebuild(self, docs):
        """Rebuild the pipeline with new documents"""
        self.reset()

        logger.info(f"Rebuilding retrieval pipeline with {len(docs)} documents")
        for i, doc in enumerate(docs):
            logger.info(f"Document {i}: {doc.metadata.get('source', 'No source')} - First 100 chars: {doc.page_content[:100]}...")

        doc_splits = text_splitter.split_documents(docs)
        logger.info(f"Split into {len(doc_splits)} chunks")

        # Build FAISS vectorstore
        self.embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
        self.vectorstore = FAISS.from_documents(
            documents=doc_splits,
            embedding=self.embeddings
        )

        self.keyword_retriever = BM25Retriever.from_documents(doc_splits, similarity_top_k=20)
        vector_retriever = self.vectorstore.as_retriever(
            search_type="similarity",  
            search_kwargs={"k": 20},    
        )

        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[vector_retriever, self.keyword_retriever], 
            weights=[0.7, 0.3]  
        )
        self.documents = docs

    def has_documents(self):
        """Check if documents are loaded and retrievable"""
        return (
            len(self.documents) > 0 and 
            self.vectorstore is not None and 
            self.ensemble_retriever is not None
        )

    def retrieve(self, question: str) -> List[str]:
        """Enhanced retrieve method with logging"""
        if not self.has_documents():
            logger.warning("No documents loaded in retrieval pipeline")
            raise ValueError("No documents are currently loaded. Please upload documents first.")
        
        logger.info(f"Retrieving documents for question: {question}")
        docs = self.ensemble_retriever.invoke(question)
        logger.info(f"Retrieved {len(docs)} documents")
        
        # Extract relevant snippets instead of full documents
        relevant_snippets = []
        for doc in docs:
            score = 1.0
            logger.info(f"Processing document with score: {score}")

            content = doc.page_content
            logger.info(f"Document content preview: {content[:100]}...")

            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

            question_words = set(question.lower().split())
            relevant_parts = []
            
            for paragraph in paragraphs:
                if (len(question_words & set(paragraph.lower().split())) > 1 or
                    any(term in paragraph.lower() for term in [
                        'study', 'research', 'method', 'result', 'conclusion',
                        'analysis', 'finding', 'data', 'experiment'
                    ])):
                    relevant_parts.append(paragraph)
            
            if relevant_parts:
                # Join with newlines to maintain formatting
                snippet = '\n\n'.join(relevant_parts)
                relevant_snippets.append(snippet)
                logger.info(f"Added relevant snippet: {snippet[:100]}...")
        
        if not relevant_snippets:
            if docs:
                first_doc = docs[0].page_content
                relevant_snippets = [first_doc[:1000]]
                logger.info("No specific snippets found, using document introduction")
            else:
                logger.warning("No relevant snippets or documents found")
        
        return relevant_snippets

    def query(self, question: str) -> List[str]:
        """Alias for retrieve method to maintain compatibility"""
        return self.retrieve(question)

    def save_vectorstore(self, path: str):
        """Save the FAISS vectorstore to disk"""
        if self.vectorstore:
            self.vectorstore.save_local(path)

    def load_vectorstore(self, path: str):
        """Load the FAISS vectorstore from disk"""
        if os.path.exists(path):
            embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
            self.vectorstore = FAISS.load_local(path, embeddings)
            vector_retriever = self.vectorstore.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={"score_threshold": 0.5, "k": 2},
            )
            if self.documents:  # Only recreate ensemble if we have documents
                keyword_retriever = BM25Retriever.from_documents(
                    text_splitter.split_documents(self.documents), 
                    similarity_top_k=2
                )
                self.ensemble_retriever = EnsembleRetriever(
                    retrievers=[vector_retriever, keyword_retriever],
                    weights=[0.2, 0.8]
                )


retrieval_pipeline = RetrievalPipeline()

```


### ğŸ“„ src/services/tracing_service.py
```py
from langfuse import Langfuse
from langfuse.callback import CallbackHandler
from langfuse.decorators import langfuse_context, observe
from loguru import logger
from typing import Optional, Dict, Any, ContextManager, List
from contextlib import contextmanager
from src.utils.config import (
    LANGFUSE_PUBLIC_KEY,
    LANGFUSE_SECRET_KEY,
    LANGFUSE_HOST
)

class TracingService:
    def __init__(self):
        try:
            self.langfuse = Langfuse(
                public_key=LANGFUSE_PUBLIC_KEY,
                secret_key=LANGFUSE_SECRET_KEY,
                host=LANGFUSE_HOST
            )
            self.handler = CallbackHandler()
            logger.info("Tracing service initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize tracing service: {e}")
            raise

    @contextmanager
    def trace_interaction(self, 
                         interaction_type: str, 
                         metadata: Optional[Dict] = None,
                         tags: Optional[list] = None) -> ContextManager:
        """Start a new trace for an interaction"""
        try:
            # Create a new trace
            trace = self.langfuse.trace(
                name=f"{interaction_type}_interaction",
                metadata=metadata or {},
                tags=tags or []
            )
            
            # Create a span
            span = trace.span(name=interaction_type)
            if metadata:
                span.metadata = metadata
            if tags:
                span.tags = tags
            
            yield span
            
        except Exception as e:
            logger.error(f"Error in trace context: {e}")
            yield None

    def get_handler(self) -> CallbackHandler:
        """Get the Langfuse callback handler for LangChain"""
        return self.handler

    def add_score(self, 
                 name: str, 
                 value: float, 
                 comment: Optional[str] = None,
                 trace_id: Optional[str] = None) -> None:
        """Add a score to a trace"""
        try:
            if trace_id:
                self.langfuse.score(
                    trace_id=trace_id,
                    name=name,
                    value=value,
                    comment=comment
                )
            else:
                # Create a new trace for the score
                trace = self.langfuse.trace(
                    name=f"score_{name}",
                    metadata={"score_name": name}
                )
                span = trace.span(name="add_score")
                span.score(
                    name=name,
                    value=value,
                    comment=comment
                )
                    
            logger.debug(f"Added score: {name}={value}")
        except Exception as e:
            logger.error(f"Error adding score: {e}")

    def log_memory_access(self, thread_id: str, message_count: int, tags: Optional[List[str]] = None) -> None:
        """Log memory access"""
        try:
            trace = self.langfuse.trace(
                name="memory_access",
                metadata={
                    "thread_id": thread_id,
                    "message_count": message_count
                },
                tags=tags or ["memory", "history", "conversation"]
            )
            span = trace.span(name="memory_access")
            span.score(
                name="memory_access",
                value=1.0,
                comment=f"Retrieved {message_count} messages"
            )
                
        except Exception as e:
            logger.error(f"Error logging memory access: {e}")

    def flush(self):
        """Flush any pending traces"""
        try:
            self.langfuse.flush()
        except Exception as e:
            logger.error(f"Error flushing traces: {e}")

    def shutdown(self):
        """Shutdown the tracing service"""
        try:
            self.flush()
            self.langfuse.shutdown()
        except Exception as e:
            logger.error(f"Error shutting down tracing service: {e}")

tracing_service = TracingService() 
```

```


### ğŸ“„ docker-compose.yml
```yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src
      - ./papers:/app/src/docs
      - vectorstore_data:/app/src/data/vectorstore
    env_file:
      - .env
    environment:
      - PYTHONPATH=/app
      - PORT=8000
    restart: unless-stopped

volumes:
  vectorstore_data:
```


### ğŸ“„ postman_test_cases.json
```json
{
  "info": {
    "name": "Research Paper Agent API Tests",
    "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
  },
  "item": [
    {
      "name": "1. Initial Chat - Who are you?",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/query",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n    \"question\": \"Who are you?\",\n    \"thread_id\": null,\n    \"config\": null\n}",
          "options": {
            "raw": {
              "language": "json"
            }
          }
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Response structure is valid\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData).to.have.property('answer');",
              "    pm.expect(jsonData).to.have.property('thread_id');",
              "    pm.expect(jsonData).to.have.property('references');",
              "    pm.collectionVariables.set(\"thread_id\", jsonData.thread_id);",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "2. Ask About Bangkok",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/query",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n    \"question\": \"What is capital of Thailand?\",\n    \"thread_id\": \"{{thread_id}}\",\n    \"config\": null\n}",
          "options": {
            "raw": {
              "language": "json"
            }
          }
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Answer mentions Bangkok\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.answer.toLowerCase()).to.include('bangkok');",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "3. Memory Test - Previous Question",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/query",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n    \"question\": \"What was my previous question?\",\n    \"thread_id\": \"{{thread_id}}\",\n    \"config\": null\n}",
          "options": {
            "raw": {
              "language": "json"
            }
          }
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Answer mentions Thailand or capital\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.answer.toLowerCase()).to.satisfy(function(text) {",
              "        return text.includes('thailand') || text.includes('capital');",
              "    });",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "4. Upload Research Papers",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/docs/upload",
        "header": [],
        "body": {
          "mode": "formdata",
          "formdata": [
            {
              "key": "files",
              "type": "file",
              "src": [
                "papers/paper1.pdf",
                "papers/paper2.pdf",
                "papers/paper3.pdf",
                "papers/paper4.pdf"
              ]
            }
          ]
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Files uploaded successfully\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.message).to.include('successfully');",
              "    pm.expect(jsonData.details.successful.length).to.equal(4);",
              "    pm.expect(jsonData.file_count).to.be.greaterThan(0);",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "5. Query About Text-to-SQL Papers",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/query",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n    \"question\": \"What is conclusion of txt-2-sql paper?, which model is that best?\",\n    \"thread_id\": \"{{thread_id}}\",\n    \"config\": null\n}",
          "options": {
            "raw": {
              "language": "json"
            }
          }
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Response includes references\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.references).to.be.an('array');",
              "    pm.expect(jsonData.references.length).to.be.greaterThan(0);",
              "    pm.expect(jsonData.references[0]).to.have.all.keys('source', 'relevance_score', 'snippet');",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "6. Reset Thread",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/thread/reset?thread_id={{thread_id}}",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ]
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Thread reset successful\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.status).to.equal('success');",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "7. Ask About Previous Question After Reset",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/query",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n    \"question\": \"What did I just ask?\",\n    \"thread_id\": \"{{thread_id}}\",\n    \"config\": null\n}",
          "options": {
            "raw": {
              "language": "json"
            }
          }
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Response indicates no history\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.answer.toLowerCase()).to.include('no previous');",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "8. Reset Vector Database",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/vectordb/reset",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ]
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Vector DB reset successful\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.status).to.equal('success');",
              "    pm.expect(jsonData.has_documents).to.equal(false);",
              "});"
            ]
          }
        }
      ]
    },
    {
      "name": "9. Query About LLM Self-Debug",
      "request": {
        "method": "POST",
        "url": "{{base_url}}/api/query",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n    \"question\": \"Can LLMs benefit from multi-round self debug?\",\n    \"thread_id\": \"{{thread_id}}\",\n    \"config\": null\n}",
          "options": {
            "raw": {
              "language": "json"
            }
          }
        }
      },
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "pm.test(\"Status code is 200\", function () {",
              "    pm.response.to.have.status(200);",
              "});",
              "",
              "pm.test(\"Response indicates no documents\", function () {",
              "    var jsonData = pm.response.json();",
              "    pm.expect(jsonData.answer.toLowerCase()).to.include('no documents');",
              "    pm.expect(jsonData.references).to.be.an('array').that.is.empty;",
              "});"
            ]
          }
        }
      ]
    }
  ],
  "variable": [
    {
      "key": "base_url",
      "value": "http://localhost:8000"
    },
    {
      "key": "thread_id",
      "value": ""
    }
  ]
}

```


### ğŸ“„ requirements.txt
```txt
# FastAPI and Server
fastapi==0.115.6
uvicorn==0.34.0
python-multipart==0.0.20

# Environment Management
python-dotenv==1.0.1

# Logging
loguru==0.7.3

# Pydantic (Data Validation and Parsing)
pydantic==2.10.4

# LangChain and Related Libraries
langchain==0.3.13
langchain-core==0.3.28
langchain-community==0.3.13
langchain-openai==0.2.14
langchain_together==0.2.0
langgraph==0.2.60
langfuse==2.57.0

# Natural Language Processing and Search
faiss-cpu==1.9.0.post1
rank_bm25==0.2.2

# PDF Processing
pypdf==5.1.0


```


### ğŸ“„ tests/conftest.py
```py
import os
import pytest
from fastapi.testclient import TestClient
from src.main import app
from src.services.graph_service import GraphService
from src.services.llm_service import LLMService
from src.services.memory_service import MemoryService
from src.services.retrieval_service import RetrievalPipeline
from langchain.schema import Document

@pytest.fixture(scope="session", autouse=True)
def setup_test_env():
    """Setup test environment"""
    fixtures_dir = os.path.join(os.path.dirname(__file__), "fixtures")
    os.makedirs(fixtures_dir, exist_ok=True)

    sample_pdf_path = os.path.join(fixtures_dir, "sample.pdf")
    if not os.path.exists(sample_pdf_path):
        with open(sample_pdf_path, "wb") as f:
            f.write(b"%PDF-1.4\n1 0 obj\n<<\n/Type /Catalog\n>>\nendobj\ntrailer\n<<\n/Root 1 0 R\n>>\n%%EOF")

@pytest.fixture
def test_client():
    return TestClient(app)

@pytest.fixture
def sample_pdf_path():
    return os.path.join(os.path.dirname(__file__), "fixtures", "sample.pdf")

@pytest.fixture
def mock_documents():
    return [
        Document(
            page_content="This is a test document about machine learning.",
            metadata={"source": "test1.pdf"}
        ),
        Document(
            page_content="This document discusses neural networks in detail.",
            metadata={"source": "test2.pdf"}
        )
    ]

@pytest.fixture
def graph_service():
    return GraphService()

@pytest.fixture
def llm_service():
    return LLMService()

@pytest.fixture
def memory_service():
    return MemoryService()

@pytest.fixture
def retrieval_pipeline():
    pipeline = RetrievalPipeline()
    yield pipeline
    pipeline.reset()  # Cleanup after each test 
```


### ğŸ“„ tests/unit/test_graph_service.py
```py
import pytest
from src.services.graph_service import GraphService
from langchain.schema import HumanMessage, AIMessage

def test_process_question_no_documents(graph_service):
    """Test processing question when no documents are loaded"""
    response = graph_service.process_question("What is AI?")
    assert "don't have any research papers loaded" in response.lower()

def test_process_question_with_history(graph_service, mock_documents):
    """Test processing question with conversation history"""
    # First rebuild the retrieval pipeline
    from src.services.retrieval_service import retrieval_pipeline
    retrieval_pipeline.rebuild(mock_documents)
    
    history = [
        HumanMessage(content="What is machine learning?"),
        AIMessage(content="Machine learning is a subset of AI.")
    ]
    response = graph_service._process_with_memory(
        "Can you elaborate on that?",
        history
    )
    assert response is not None
    assert isinstance(response, str)

@pytest.mark.parametrize("recursion_count,expected_contains", [
    (5, "steps"),
    (10, "steps"),
    (15, "steps"),
])
def test_format_error_response(graph_service, recursion_count, expected_contains):
    """Test error response formatting"""
    response = graph_service.format_error_response("Test error", recursion_count)
    assert expected_contains in response.lower()
```


### ğŸ“„ tests/unit/test_llm_service.py
```py
import pytest
from src.services.llm_service import LLMService

def test_llm_initialization():
    """Test LLM service initialization"""
    service = LLMService()
    assert service.model == "gpt-4o-mini"
    assert service.temperature == 0

def test_llm_invoke_with_system_prompt(llm_service):
    """Test LLM invocation with system prompt"""
    response = llm_service.invoke("What is AI?")
    assert isinstance(response, str)
    assert len(response) > 0

@pytest.mark.parametrize("config", [
    {"temperature": 0.7},
    {"max_tokens": 100},
    {"temperature": 0, "max_tokens": 50},
])
def test_llm_invoke_with_config(llm_service, config):
    """Test LLM invocation with different configs"""
    response = llm_service.invoke("Test prompt", config)
    assert isinstance(response, str) 
```


### ğŸ“„ tests/unit/test_memory_service.py
```py
import pytest
from src.services.memory_service import MemoryService
from langchain.schema import HumanMessage, AIMessage

def test_add_and_get_messages(memory_service):
    """Test adding and retrieving messages"""
    thread_id = "test-thread"
    message1 = HumanMessage(content="Test question")
    message2 = AIMessage(content="Test answer")
    
    memory_service.add_message(thread_id, message1)
    memory_service.add_message(thread_id, message2)
    
    messages = memory_service.get_messages(thread_id)
    assert len(messages) == 2
    assert messages[0].content == "Test question"
    assert messages[1].content == "Test answer"

def test_get_last_k_messages(memory_service):
    """Test retrieving last K messages"""
    thread_id = "test-thread"
    for i in range(5):
        memory_service.add_message(
            thread_id,
            HumanMessage(content=f"Message {i}")
        )
    
    messages = memory_service.get_messages(thread_id, last_k=3)
    assert len(messages) == 3
    assert messages[-1].content == "Message 4"

def test_clear_thread(memory_service):
    """Test clearing a conversation thread"""
    thread_id = "test-thread"
    memory_service.add_message(
        thread_id,
        HumanMessage(content="Test")
    )
    
    memory_service.clear_thread(thread_id)
    assert not memory_service.thread_exists(thread_id) 
```


### ğŸ“„ tests/unit/test_retrieval_service.py
```py
import pytest
from src.services.retrieval_service import RetrievalPipeline

def test_pipeline_initialization(retrieval_pipeline):
    """Test retrieval pipeline initialization"""
    assert retrieval_pipeline.vectorstore is None
    assert retrieval_pipeline.documents == []

def test_pipeline_rebuild(retrieval_pipeline, mock_documents):
    """Test rebuilding pipeline with documents"""
    retrieval_pipeline.rebuild(mock_documents)
    assert retrieval_pipeline.has_documents()
    assert len(retrieval_pipeline.documents) == 2

def test_retrieve_with_no_documents(retrieval_pipeline):
    """Test retrieval with no documents loaded"""
    with pytest.raises(ValueError):
        retrieval_pipeline.retrieve("What is AI?")

def test_retrieve_with_documents(retrieval_pipeline, mock_documents):
    """Test document retrieval"""
    retrieval_pipeline.rebuild(mock_documents)
    results = retrieval_pipeline.retrieve("machine learning")
    assert len(results) > 0
    assert "machine learning" in results[0].lower()

@pytest.mark.parametrize("question,expected_count", [
    ("machine learning", 1),
    ("neural networks", 1),
    ("irrelevant query", 0),
])
def test_retrieve_different_queries(
    retrieval_pipeline,
    mock_documents,
    question,
    expected_count
):
    """Test retrieval with different queries"""
    retrieval_pipeline.rebuild(mock_documents)
    results = retrieval_pipeline.retrieve(question)
    assert len([r for r in results if question.lower() in r.lower()]) >= expected_count 
```


### ğŸ“„ scripts/export_codebase.py
```py
import os
from pathlib import Path
import json

def get_file_content(file_path: str) -> str:
    """Read and return file content with proper error handling"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        return f"Error reading file: {str(e)}"

def should_ignore(path: str) -> bool:
    """Check if path should be ignored"""
    ignore_patterns = [
        '__pycache__',
        '.git',
        '.env',
        '.venv',
        'node_modules',
        '.pytest_cache',
        '.coverage',
        '.idea',
        '.vscode',
        'dist',
        'build',
        '*.pyc',
        '*.pyo',
        '*.pyd',
        '.DS_Store',
    ]
    
    return any(pattern in path for pattern in ignore_patterns)

def export_codebase(root_dir: str, output_file: str):
    """Export codebase structure and content to a text file"""
    
    # Get project name from root directory
    project_name = os.path.basename(os.path.abspath(root_dir))
    
    # Initialize output content
    output = [
        f"# {project_name} Codebase Export",
        "\n## Project Structure\n",
    ]
    
    # Track all files for content export
    files_to_export = []
    
    # First, build directory structure
    for root, dirs, files in os.walk(root_dir):
        if should_ignore(root):
            continue
            
        # Calculate relative path and indent level
        rel_path = os.path.relpath(root, root_dir)
        indent = '  ' * (len(Path(rel_path).parts) - 1)
        
        # Add directory to structure
        if rel_path != '.':
            output.append(f"{indent}ğŸ“ {os.path.basename(root)}/")
        
        # Add files to structure and track for content export
        for file in sorted(files):
            if file.endswith(('.py', '.txt', '.json', '.yml', '.yaml', '.md', '.env.example')):
                file_path = os.path.join(root, file)
                rel_file_path = os.path.relpath(file_path, root_dir)
                output.append(f"{indent}  ğŸ“„ {file}")
                files_to_export.append(rel_file_path)
    
    # Add file contents
    output.append("\n## File Contents\n")
    
    for file_path in files_to_export:
        abs_path = os.path.join(root_dir, file_path)
        output.extend([
            f"\n### ğŸ“„ {file_path}",
            "```" + (file_path.split('.')[-1] if '.' in file_path else ''),
            get_file_content(abs_path),
            "```\n"
        ])
    
    # Write to output file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(output))

if __name__ == "__main__":
    # Get the project root directory (assuming this script is in a scripts folder)
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_path = os.path.join(project_root, "codebase_export.txt")
    
    export_codebase(project_root, output_path)
    print(f"Codebase exported to: {output_path}") 
```


### ğŸ“„ src/main.py
```py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from src.routers import query_router, docs_router
from src.utils.config import DOCS_FOLDER
from src.services.retrieval_service import retrieval_pipeline
from src.utils.pdf_utils import load_pdfs_from_directory
import uvicorn
from loguru import logger

# Create FastAPI app
app = FastAPI(title="Research Paper Agent API")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount routers
app.include_router(query_router.router)
app.include_router(docs_router.router)

@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    logger.info("Starting Research Paper Agent API")
    
    # Pre-load documents if any exist
    try:
        initial_docs = load_pdfs_from_directory(DOCS_FOLDER)
        if initial_docs:
            retrieval_pipeline.rebuild(initial_docs)
            logger.info(f"Loaded {len(initial_docs)} documents on startup")
    except Exception as e:
        logger.warning(f"Error loading initial documents: {e}")

@app.get("/")
async def root():
    """Root endpoint to check API status"""
    return {
        "status": "running",
        "message": "Research Paper Agent API is running",
        "docs_url": "/docs",
        "openapi_url": "/openapi.json"
    }

def run_server():
    """Run the FastAPI server"""
    uvicorn.run(
        "src.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,  # Enable auto-reload during development
        log_level="info"
    )

if __name__ == "__main__":
    run_server()

```


### ğŸ“„ src/routers/docs_router.py
```py
from fastapi import APIRouter, UploadFile, File, HTTPException
from typing import List
from src.services.retrieval_service import retrieval_pipeline
from src.utils.pdf_utils import process_uploaded_pdf
from loguru import logger

router = APIRouter(prefix="/api")

@router.post("/docs/upload")
async def upload_documents(files: List[UploadFile] = File(...)):
    """Upload multiple PDF documents to be processed and added to the knowledge base"""
    try:
        if not files:
            raise HTTPException(status_code=400, detail="No files provided")

        results = {
            "successful": [],
            "failed": []
        }
        
        all_docs = []

        for file in files:
            try:
                if not file.filename.endswith('.pdf'):
                    results["failed"].append({
                        "filename": file.filename,
                        "error": "Only PDF files are supported"
                    })
                    continue

                docs = await process_uploaded_pdf(file)
                if docs:
                    all_docs.extend(docs)
                    results["successful"].append({
                        "filename": file.filename,
                        "pages": len(docs)
                    })
                else:
                    results["failed"].append({
                        "filename": file.filename,
                        "error": "No content could be extracted"
                    })
                    
            except Exception as e:
                logger.error(f"Error processing file {file.filename}: {e}")
                results["failed"].append({
                    "filename": file.filename,
                    "error": str(e)
                })

        if all_docs:
            retrieval_pipeline.rebuild(all_docs)
            logger.info(f"Rebuilt retrieval pipeline with {len(all_docs)} documents")

        response = {
            "message": f"Processed {len(results['successful'])} files successfully, {len(results['failed'])} failed",
            "details": results,
            "file_count": len(all_docs)
        }
        
        # If all files failed, return 400
        if not results["successful"] and results["failed"]:
            raise HTTPException(status_code=400, detail=response)
            
        return response
        
    except Exception as e:
        logger.exception(f"Error in upload_documents: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/status")
async def get_document_status():
    """Get the status of loaded documents"""
    try:
        has_docs = retrieval_pipeline.has_documents()
        return {
            "has_documents": has_docs,
            "document_count": len(retrieval_pipeline.documents) if has_docs else 0
        }
    except Exception as e:
        logger.exception(f"Error getting document status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

```


### ğŸ“„ src/routers/query_router.py
```py
from fastapi import APIRouter, Query, HTTPException, Depends
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field
from typing import Literal, Optional, List
from src.services.memory_service import memory_service
from src.services.llm_service import llm_service
from src.services.graph_service import graph_service
from src.services.retrieval_service import RetrievalPipeline, retrieval_pipeline
import uuid
from loguru import logger
from langchain.schema import HumanMessage, AIMessage
from enum import Enum
from src.services.tracing_service import tracing_service
from src.models.request_models import QueryRequest
from src.models.response_models import QueryResponse, ErrorResponse, DocumentReference

router = APIRouter(prefix="/api")

class QueryType(str, Enum):
    MEMORY = "memory"
    VECTORSTORE = "vectorstore"
    GENERAL = "general"

class RouteQuery(BaseModel):
    """Route a user query to the most appropriate processing method."""
    query_type: Literal["memory", "vectorstore", "general"] = Field(
        ...,
        description="Type of query processing needed"
    )
    reason: str = Field(
        ...,
        description="Explanation for routing decision"
    )

class MemoryResponse(BaseModel):
    """Structure the response for memory-related queries."""
    response_type: Literal["count", "list", "history"] = Field(
        ...,
        description="Type of memory response"
    )
    content: str = Field(
        ...,
        description="The formatted response content"
    )

# Setup LLM and Chains
llm = ChatOpenAI(
    model="gpt-4o-mini", 
    temperature=0,
    callbacks=[tracing_service.get_handler()]
)
structured_router = llm.with_structured_output(RouteQuery)
memory_chain = llm.with_structured_output(MemoryResponse)

# Router Prompt
router_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an advanced AI assistant specialized in routing user questions to appropriate handlers within a complex information system.

Your goal is to route this question to one of the following handlers:
1. 'memory': For questions about conversation history, previous questions, or interaction counts.
2. 'vectorstore': For questions requiring research paper knowledge, especially those about:
   - Technical topics (LLMs, Deep Learning, NLP)
   - Research findings or conclusions
   - Specific papers or studies
   - State-of-the-art methods or models
   - Technical comparisons or benchmarks
3. 'general': For basic questions not requiring special context, including:
   - Questions about your capabilities
   - Basic factual questions
   - Non-technical queries
   - Questions about general concepts

IMPORTANT: Any question asking about research findings, papers, or technical details MUST go to 'vectorstore'."""),
    ("human", "{question}")
])

memory_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are an expert at analyzing conversation history questions.
    Determine the appropriate response type:
    - 'count' for questions about number of interactions
    - 'list' for requests to see previous questions
    - 'history' for general conversation history requests
    
    Format the response appropriately based on the type."""),
    ("human", """Question: {question}
    Conversation History: {history}
    
    Determine response type and format appropriate response.""")
])

router_chain = router_prompt | structured_router
memory_chain = memory_prompt | memory_chain

def handle_memory_question(question: str, thread_id: str, tags: Optional[List[str]] = None) -> dict:
    """Handle questions about conversation history"""
    messages = memory_service.get_messages(thread_id)
    history = "\n".join([f"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}" 
                        for m in messages])

    memory_result = memory_chain.invoke(
        {
            "question": question,
            "history": history
        },
        config={
            "callbacks": [tracing_service.get_handler()],
            "tags": tags or ["memory", "history", "conversation"]
        }
    )

    memory_service.add_message(thread_id, HumanMessage(content=question))
    memory_service.add_message(thread_id, AIMessage(content=memory_result.content))
    
    return {
        "response": memory_result.content,
        "thread_id": thread_id,
        "source": "memory",
        "response_type": memory_result.response_type
    }

def handle_general_question(question: str, thread_id: str, tags: Optional[List[str]] = None) -> dict:
    """Handle general questions using LLM"""
    response = llm_service.invoke(
        question,
        config={
            "callbacks": [tracing_service.get_handler()],
            "tags": tags or ["general", "llm", "direct"]
        }
    )
    memory_service.add_message(thread_id, HumanMessage(content=question))
    memory_service.add_message(thread_id, AIMessage(content=response))
    
    return {
        "response": response,
        "thread_id": thread_id,
        "source": "general"
    }

@router.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """Process a query with document references"""
    try:
        thread_id = request.thread_id or str(uuid.uuid4())
        
        # Route the question
        route = router_chain.invoke(
            {"question": request.question},
            config={"callbacks": [tracing_service.get_handler()]}
        )
        
        logger.info(f"Question routed to: {route.query_type}")
        
        if route.query_type == "memory":
            result = handle_memory_question(request.question, thread_id)
            return QueryResponse(
                answer=result["response"],
                references=[],
                thread_id=thread_id
            )
            
        elif route.query_type == "general":
            result = handle_general_question(request.question, thread_id)
            return QueryResponse(
                answer=result["response"],
                references=[],
                thread_id=thread_id
            )
            
        elif route.query_type == "vectorstore":
            if not retrieval_pipeline.has_documents():
                return QueryResponse(
                    answer="I cannot answer research questions as no documents have been loaded. Please upload some research papers first.",
                    references=[],
                    thread_id=thread_id
                )
                
            try:
                # Initialize graph state
                state = {
                    "question": request.question,
                    "documents": [],
                    "rewritten_question": None,
                    "graded_docs": None,
                    "answer": None,
                    "thread_id": thread_id
                }
                
                # Get initial documents
                retrieved_docs = retrieval_pipeline.retrieve(request.question)
                if not retrieved_docs:
                    return QueryResponse(
                        answer="I couldn't find any relevant information in the documents.",
                        references=[],
                        thread_id=thread_id
                    )
                
                # Add retrieved docs to state
                state["documents"] = [
                    {
                        "content": doc["content"],
                        "source": doc["source"],
                        "score": doc["score"]
                    } for doc in retrieved_docs
                ]
                
                # Process through graph service
                try:
                    graph_result = graph_service.process_state(state)
                    
                    # Format references from graded documents
                    references = []
                    if graph_result.get("relevant_docs"):
                        references = [
                            DocumentReference(
                                source=doc["source"].split('/')[-1],
                                relevance_score=doc["grade"],
                                snippet=doc["content"][:500]
                            )
                            for doc in graph_result["relevant_docs"]
                        ]
                    
                    # Get final answer
                    answer = graph_result.get("answer", "Could not generate an answer from the documents.")
                    
                    # Add to conversation history
                    memory_service.add_message(thread_id, HumanMessage(content=request.question))
                    memory_service.add_message(thread_id, AIMessage(content=answer))
                    
                    return QueryResponse(
                        answer=answer,
                        references=sorted(references, key=lambda x: x.relevance_score, reverse=True),
                        thread_id=thread_id
                    )
                    
                except Exception as graph_error:
                    logger.error(f"Graph processing error: {graph_error}")
                    # Fallback to simpler processing
                    return handle_research_fallback(request.question, retrieved_docs, thread_id)
                
            except Exception as e:
                logger.error(f"Error processing research question: {e}")
                raise HTTPException(status_code=400, detail=str(e))
        
    except Exception as e:
        logger.exception(f"Error processing query: {e}")
        raise HTTPException(status_code=400, detail=str(e))

def handle_research_fallback(question: str, docs: List[dict], thread_id: str) -> QueryResponse:
    """Fallback handler for research questions when graph processing fails"""
    try:
        context = "\n\n".join([
            f"From {doc['source']}:\n{doc['content']}"
            for doc in docs
        ])
        
        focused_question = f"""Based ONLY on the provided research papers, answer the following question: {question}
        Please provide a clear and concise answer, focusing on key findings and conclusions."""
        
        answer = llm_service.invoke(
            f"Context from papers:\n{context}\n\nQuestion: {focused_question}",
            config={"temperature": 0.2}
        )
        
        references = [
            DocumentReference(
                source=doc['source'].split('/')[-1],
                relevance_score=doc['score'],
                snippet=doc['content'][:500]
            )
            for doc in docs
        ]
        
        memory_service.add_message(thread_id, HumanMessage(content=question))
        memory_service.add_message(thread_id, AIMessage(content=answer))
        
        return QueryResponse(
            answer=answer,
            references=references,
            thread_id=thread_id
        )
        
    except Exception as e:
        logger.error(f"Error in research fallback: {e}")
        raise

@router.post("/thread/reset")
async def reset_thread(thread_id: str = Query(..., description="Thread ID to reset")):
    """Reset a conversation thread"""
    try:
        if not memory_service.thread_exists(thread_id):
            raise HTTPException(
                status_code=404, 
                detail=f"Thread {thread_id} not found"
            )
            
        memory_service.clear_thread(thread_id)
        logger.info(f"Thread {thread_id} reset successfully")
        return {
            "status": "success",
            "message": "Thread reset successfully",
            "thread_id": thread_id
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Error in reset_thread")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/vectordb/reset")
async def reset_vectordb():
    """Reset the vector database"""
    try:
        # Reset the existing pipeline instead of creating new one
        retrieval_pipeline.reset()
        
        logger.info("Vector database reset successfully")
        return {
            "status": "success",
            "message": "Vector database reset successfully",
            "document_count": 0,
            "has_documents": False
        }
    except Exception as e:
        logger.exception("Error in reset_vectordb")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/thread/status/{thread_id}")
async def get_thread_status(thread_id: str):
    """Get the status of a conversation thread"""
    try:
        messages = memory_service.get_messages(thread_id)
        return {
            "thread_id": thread_id,
            "message_count": len(messages),
            "has_history": len(messages) > 0,
            "messages": [
                {
                    "type": "human" if isinstance(msg, HumanMessage) else "ai",
                    "content": msg.content
                }
                for msg in messages
            ]
        }
    except Exception as e:
        logger.exception("Error getting thread status")
        raise HTTPException(status_code=500, detail=str(e))

```


### ğŸ“„ src/utils/config.py
```py
import os
from dotenv import load_dotenv

load_dotenv()

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Together AI
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")

# Langfuse
LANGFUSE_PUBLIC_KEY = os.getenv("LANGFUSE_PUBLIC_KEY")
LANGFUSE_SECRET_KEY = os.getenv("LANGFUSE_SECRET_KEY")
LANGFUSE_HOST = os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")

# Docs
DOCS_FOLDER = "src/docs"

DEFAULT_USER_ID = "default_user"
DEFAULT_THREAD_ID = "default_thread"
PRIMARY_LLM_MODEL = "meta-llama/Llama-3.3-70B-Instruct-Turbo"
BACKUP_LLM_MODEL = "gpt-4o-mini"

```


### ğŸ“„ src/utils/logging.py
```py
from loguru import logger
import sys

# Configure loguru
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO"
)

# Add file logging
logger.add(
    "logs/app.log",
    rotation="500 MB",
    retention="10 days",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    level="DEBUG"
)

```


### ğŸ“„ src/utils/pdf_utils.py
```py
import os
from typing import List
from fastapi import UploadFile
from langchain.docstore.document import Document
from langchain_community.document_loaders import PyPDFLoader
from src.utils.config import DOCS_FOLDER

async def process_uploaded_pdf(file: UploadFile) -> List[Document]:
    """
    Process an uploaded PDF file and return the extracted documents
    
    Args:
        file (UploadFile): The uploaded PDF file
        
    Returns:
        List[Document]: List of extracted documents from the PDF
    """
    # Create the docs directory if it doesn't exist
    os.makedirs(DOCS_FOLDER, exist_ok=True)
    
    # Save the uploaded file
    file_path = os.path.join(DOCS_FOLDER, file.filename)
    content = await file.read()
    
    with open(file_path, "wb") as f:
        f.write(content)
    
    try:
        # Load and process the PDF
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        
        # Concatenate pages belonging to the same document
        if documents:
            combined_doc = documents[0]
            for doc in documents[1:]:
                combined_doc.page_content += "\n\n" + doc.page_content
            return [combined_doc]
        
        return []
        
    except Exception as e:
        # Clean up the file if processing fails
        if os.path.exists(file_path):
            os.remove(file_path)
        raise Exception(f"Error processing PDF: {str(e)}")

def load_pdfs_from_directory(directory: str) -> List[Document]:
    """
    Load all PDFs from a directory
    
    Args:
        directory (str): Path to directory containing PDFs
        
    Returns:
        List[Document]: List of documents from all PDFs
    """
    if not os.path.exists(directory):
        return []
        
    documents = []
    for filename in os.listdir(directory):
        if filename.endswith('.pdf'):
            file_path = os.path.join(directory, filename)
            try:
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                
                # Combine pages of the same document
                if docs:
                    combined_doc = docs[0]
                    for doc in docs[1:]:
                        combined_doc.page_content += "\n\n" + doc.page_content
                    documents.append(combined_doc)
                    
            except Exception as e:
                print(f"Error loading {filename}: {str(e)}")
                continue
                
    return documents

def load_pdf(file_path: str) -> List[Document]:
    loader = PyPDFLoader(file_path)
    return loader.load()

```


### ğŸ“„ src/models/request_models.py
```py
from pydantic import BaseModel, Field
from typing import Optional, Dict

class QueryRequest(BaseModel):
    question: str = Field(..., description="Question to ask")
    thread_id: Optional[str] = Field(None, description="Thread ID for conversation continuity")
    config: Optional[Dict] = Field(None, description="Additional configuration for the query")

class UploadRequest(BaseModel):
    file_type: Optional[str] = Field("pdf", description="Type of file being uploaded")

```


### ğŸ“„ src/models/response_models.py
```py
from pydantic import BaseModel
from typing import List, Optional

class DocumentReference(BaseModel):
    source: str
    relevance_score: float
    snippet: str

class QueryResponse(BaseModel):
    answer: str
    references: List[DocumentReference]
    thread_id: Optional[str] = None

class UploadResponse(BaseModel):
    message: str
    file_count: int

class ErrorResponse(BaseModel):
    error: str
    details: Optional[str] = None

```


### ğŸ“„ src/services/graph_service.py
```py
from typing import Dict, Any, List
from loguru import logger
from langchain.schema import HumanMessage, AIMessage
from src.services.llm_service import llm_service

class GraphService:
    def __init__(self, recursion_limit: int = 15):
        self.recursion_limit = recursion_limit
        logger.info(f"Initializing GraphService with recursion limit: {recursion_limit}")

    def process_state(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Process the state through the graph workflow"""
        try:
            # 1. Rewrite/improve the question
            state = self.rewrite_question(state)
            
            # 2. Grade and filter documents
            state = self.grade_documents(state)
            
            # 3. Generate answer
            state = self.generate_answer(state)
            
            return state
            
        except Exception as e:
            logger.error(f"Error in graph processing: {e}")
            raise

    def rewrite_question(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Improve the question for better document retrieval"""
        try:
            prompt = f"""Rewrite this research question to be more specific and focused: "{state['question']}"
            
            Rules:
            1. Keep it concise
            2. Use technical terminology
            3. Focus on specific aspects
            4. Maintain clarity
            
            Return ONLY the rewritten question, nothing else."""
            
            rewritten = llm_service.invoke(prompt, config={"temperature": 0.3})
            # Clean up the response - remove quotes and extra whitespace
            rewritten = rewritten.strip().strip('"').strip()
            state["rewritten_question"] = rewritten
            logger.info(f"Rewrote question: {rewritten}")
            
            return state
            
        except Exception as e:
            logger.error(f"Error rewriting question: {e}")
            state["rewritten_question"] = state["question"]
            return state

    def grade_documents(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Grade documents for relevance to the question"""
        try:
            graded_docs = []
            question = state["rewritten_question"] or state["question"]
            
            for doc in state["documents"]:
                grade_prompt = f"""Rate the relevance of this document excerpt to the question: "{question}"

                Document content:
                {doc['content'][:1000]}...

                Rate from 0-1 where:
                0: Not relevant at all
                0.5: Somewhat relevant
                1: Highly relevant

                Return ONLY the numerical score (e.g., 0.7)."""
                
                response = llm_service.invoke(grade_prompt, config={"temperature": 0.1})
                
                try:
                    # Clean up and extract numerical score
                    grade = float(response.strip())
                except:
                    grade = doc['score']
                
                graded_docs.append({
                    "content": doc["content"],
                    "source": doc["source"],
                    "grade": grade,
                    "original_score": doc["score"]
                })
            
            # Sort by grade
            graded_docs.sort(key=lambda x: x["grade"], reverse=True)
            state["graded_docs"] = graded_docs
            
            return state
            
        except Exception as e:
            logger.error(f"Error grading documents: {e}")
            state["graded_docs"] = [{
                **doc,
                "grade": doc["score"]
            } for doc in state["documents"]]
            return state

    def generate_answer(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Generate final answer using graded documents"""
        try:
            # Use only highly relevant documents
            relevant_docs = [
                doc for doc in state["graded_docs"]
                if doc["grade"] > 0.5
            ]
            
            if not relevant_docs:
                relevant_docs = state["graded_docs"][:2]  # Use top 2 if none are highly relevant
            
            context = "\n\n".join([
                f"From {doc['source']} (relevance: {doc['grade']:.2f}):\n{doc['content']}"
                for doc in relevant_docs
            ])
            
            prompt = f"""Based on these research paper excerpts, answer this question: "{state['question']}"

            Context from papers:
            {context}

            Instructions:
            1. Only use information from the provided excerpts
            2. Cite specific papers when making claims
            3. If information is incomplete or unclear, acknowledge it
            4. Focus on key findings and conclusions
            5. Be concise but thorough
            6. Include clear references to papers

            Answer:"""
            
            answer = llm_service.invoke(
                prompt,
                config={
                    "temperature": 0.2,
                    "max_tokens": 1000
                }
            )
            
            # Store both answer and relevant docs in state
            state["answer"] = answer
            state["relevant_docs"] = relevant_docs
            return state
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            raise

graph_service = GraphService()

```


### ğŸ“„ src/services/llm_service.py
```py
from langchain_openai import ChatOpenAI
from langchain_together import ChatTogether
from langchain.schema import SystemMessage
from src.utils.config import OPENAI_API_KEY, TOGETHER_API_KEY
from src.services.tracing_service import tracing_service
from loguru import logger
from typing import Optional, Dict

class LLMService:
    def __init__(self, model="meta-llama/Llama-3.3-70B-Instruct-Turbo", temperature=0):
        self.primary_model = model
        self.backup_model = "gpt-4o-mini"
        self.temperature = temperature
        self._primary_llm = None
        self._backup_llm = None
        self.system_prompt = """You are a Research Paper Agent, build for research paper chatbot.
Always maintain a professional, academic tone while being helpful and clear."""
        
        self.setup_llm()
    
    def setup_llm(self):
        """Initialize both LLMs with error handling"""
        try:
            # Setup primary LLM (Together AI)
            self._primary_llm = ChatTogether(
                model=self.primary_model,
                temperature=self.temperature,
                api_key=TOGETHER_API_KEY,
                max_retries=2,
                callbacks=[tracing_service.get_handler()]
            )
            logger.info(f"Initialized primary LLM with model: {self.primary_model}")
        except Exception as e:
            logger.error(f"Error initializing primary LLM: {e}")
            self._primary_llm = None

        try:
            # Setup backup LLM (OpenAI)
            self._backup_llm = ChatOpenAI(
                model=self.backup_model,
                temperature=self.temperature,
                api_key=OPENAI_API_KEY,
                callbacks=[tracing_service.get_handler()]
            )
            logger.info(f"Initialized backup LLM with model: {self.backup_model}")
        except Exception as e:
            logger.error(f"Error initializing backup LLM: {e}")
            self._backup_llm = None
    
    @property
    def llm(self):
        """Get best available LLM instance with fallback logic"""
        if not self._primary_llm and not self._backup_llm:
            self.setup_llm()
        
        # Try primary LLM first
        if self._primary_llm:
            try:
                # Quick test to check if primary LLM is responsive
                self._primary_llm.invoke([{"role": "user", "content": "test"}])
                return self._primary_llm
            except Exception as e:
                logger.warning(f"Primary LLM failed, falling back to backup: {e}")
        
        # Fallback to backup LLM
        if self._backup_llm:
            return self._backup_llm
        
        raise RuntimeError("No LLM service available")
    
    def invoke(self, prompt: str, config: Optional[Dict] = None) -> str:
        """Invoke LLM with error handling and fallback"""
        try:
            logger.debug(f"Invoking LLM with prompt: {prompt[:100]}...")

            messages = [
                SystemMessage(content=self.system_prompt),
                {"role": "user", "content": prompt}
            ]
            
            # Merge configs
            invoke_config = {}
            if config:
                invoke_config.update(config)
            
            # Get best available LLM
            current_llm = self.llm
            
            response = current_llm.invoke(
                messages,
                config=invoke_config
            )
            
            response_text = str(response.content) if hasattr(response, 'content') else str(response)
            logger.debug(f"LLM response: {response_text[:100]}...")
            
            # Log which model was used
            model_used = (
                self.primary_model if current_llm == self._primary_llm 
                else self.backup_model
            )
            logger.info(f"Response generated using model: {model_used}")
            
            return response_text
            
        except Exception as e:
            logger.exception(f"Error invoking LLM: {e}")
            raise

llm_service = LLMService() 
```


### ğŸ“„ src/services/memory_service.py
```py
from typing import Dict, List
from langchain_core.messages import BaseMessage
from loguru import logger

class MemoryService:
    def __init__(self):
        self.conversations: Dict[str, List[BaseMessage]] = {}
        
    def add_message(self, thread_id: str, message: BaseMessage):
        """Add a message to a conversation thread"""
        if thread_id not in self.conversations:
            self.conversations[thread_id] = []
        self.conversations[thread_id].append(message)
        logger.debug(f"Added message to thread {thread_id}. Total messages: {len(self.conversations[thread_id])}")
        
    def get_messages(self, thread_id: str, last_k: int = None) -> List[BaseMessage]:
        """Get messages from a conversation thread"""
        messages = self.conversations.get(thread_id, [])
        logger.debug(f"Retrieved {len(messages)} messages from thread {thread_id}")
        if last_k:
            messages = messages[-last_k:]
        return messages
        
    def clear_thread(self, thread_id: str):
        """Clear a conversation thread"""
        if thread_id in self.conversations:
            del self.conversations[thread_id]
            logger.info(f"Cleared thread {thread_id}")
            
    def thread_exists(self, thread_id: str) -> bool:
        """Check if a thread exists"""
        return thread_id in self.conversations

memory_service = MemoryService()

```


### ğŸ“„ src/services/retrieval_service.py
```py
import os
import logging
from typing import List
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers import EnsembleRetriever

from src.utils.config import OPENAI_API_KEY

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]  # Try to split on paragraph breaks first
)

class RetrievalPipeline:
    def __init__(self):
        self.vectorstore = None
        self.keyword_retriever = None
        self.ensemble_retriever = None
        self.documents = []
        self.embeddings = None
        
    def reset(self):
        """Completely reset the pipeline"""
        self.vectorstore = None
        self.keyword_retriever = None
        self.ensemble_retriever = None
        self.documents = []
        self.embeddings = None
        logger.info("Pipeline completely reset")
        
    def rebuild(self, docs):
        """Rebuild the pipeline with new documents"""
        self.reset()

        logger.info(f"Rebuilding retrieval pipeline with {len(docs)} documents")
        for i, doc in enumerate(docs):
            logger.info(f"Document {i}: {doc.metadata.get('source', 'No source')} - First 100 chars: {doc.page_content[:100]}...")

        doc_splits = text_splitter.split_documents(docs)
        logger.info(f"Split into {len(doc_splits)} chunks")

        # Build FAISS vectorstore
        self.embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
        self.vectorstore = FAISS.from_documents(
            documents=doc_splits,
            embedding=self.embeddings
        )

        self.keyword_retriever = BM25Retriever.from_documents(doc_splits, similarity_top_k=6)
        vector_retriever = self.vectorstore.as_retriever(
            search_type="similarity",  
            search_kwargs={"k": 6},
        )

        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[vector_retriever, self.keyword_retriever], 
            weights=[0.7, 0.3]  
        )
        self.documents = docs

    def has_documents(self):
        """Check if documents are loaded and retrievable"""
        return (
            len(self.documents) > 0 and 
            self.vectorstore is not None and 
            self.ensemble_retriever is not None
        )

    def retrieve(self, question: str) -> List[str]:
        """Enhanced retrieve method with logging"""
        if not self.has_documents():
            logger.warning("No documents loaded in retrieval pipeline")
            raise ValueError("No documents are currently loaded. Please upload documents first.")
        
        logger.info(f"Retrieving documents for question: {question}")
        docs = self.ensemble_retriever.invoke(question)
        logger.info(f"Retrieved {len(docs)} documents")
        
        # Extract relevant snippets and their sources
        relevant_content = []
        for doc in docs:
            score = 1.0
            logger.info(f"Processing document with score: {score}")

            content = doc.page_content
            source = doc.metadata.get('source', 'Unknown source')
            logger.info(f"Document content preview: {content[:100]}...")

            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

            question_words = set(question.lower().split())
            relevant_parts = []
            
            for paragraph in paragraphs:
                if (len(question_words & set(paragraph.lower().split())) > 1 or
                    any(term in paragraph.lower() for term in [
                        'study', 'research', 'method', 'result', 'conclusion',
                        'analysis', 'finding', 'data', 'experiment'
                    ])):
                    relevant_parts.append(paragraph)
            
            if relevant_parts:
                # Join with newlines to maintain formatting
                snippet = '\n\n'.join(relevant_parts)
                relevant_content.append({
                    'content': snippet,
                    'source': source,
                    'score': score
                })
                logger.info(f"Added relevant snippet: {snippet[:100]}...")
        
        if not relevant_content:
            if docs:
                first_doc = docs[0].page_content
                relevant_content = [{
                    'content': first_doc[:1000],
                    'source': docs[0].metadata.get('source', 'Unknown source'),
                    'score': 1.0
                }]
                logger.info("No specific snippets found, using document introduction")
            else:
                logger.warning("No relevant snippets or documents found")
        
        return relevant_content

    def query(self, question: str) -> List[str]:
        """Alias for retrieve method to maintain compatibility"""
        return self.retrieve(question)

    def save_vectorstore(self, path: str):
        """Save the FAISS vectorstore to disk"""
        if self.vectorstore:
            self.vectorstore.save_local(path)

    def load_vectorstore(self, path: str):
        """Load the FAISS vectorstore from disk"""
        if os.path.exists(path):
            embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
            self.vectorstore = FAISS.load_local(path, embeddings)
            vector_retriever = self.vectorstore.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={"score_threshold": 0.5, "k": 2},
            )
            if self.documents:  # Only recreate ensemble if we have documents
                keyword_retriever = BM25Retriever.from_documents(
                    text_splitter.split_documents(self.documents), 
                    similarity_top_k=2
                )
                self.ensemble_retriever = EnsembleRetriever(
                    retrievers=[vector_retriever, keyword_retriever],
                    weights=[0.2, 0.8]
                )


retrieval_pipeline = RetrievalPipeline()

```


### ğŸ“„ src/services/tracing_service.py
```py
from langfuse import Langfuse
from langfuse.callback import CallbackHandler
from langfuse.decorators import langfuse_context, observe
from loguru import logger
from typing import Optional, Dict, Any, ContextManager, List
from contextlib import contextmanager
from src.utils.config import (
    LANGFUSE_PUBLIC_KEY,
    LANGFUSE_SECRET_KEY,
    LANGFUSE_HOST
)

class TracingService:
    def __init__(self):
        try:
            self.langfuse = Langfuse(
                public_key=LANGFUSE_PUBLIC_KEY,
                secret_key=LANGFUSE_SECRET_KEY,
                host=LANGFUSE_HOST
            )
            self.handler = CallbackHandler()
            logger.info("Tracing service initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize tracing service: {e}")
            raise

    @contextmanager
    def trace_interaction(self, 
                         interaction_type: str, 
                         metadata: Optional[Dict] = None,
                         tags: Optional[list] = None) -> ContextManager:
        """Start a new trace for an interaction"""
        try:
            # Create a new trace
            trace = self.langfuse.trace(
                name=f"{interaction_type}_interaction",
                metadata=metadata or {},
                tags=tags or []
            )
            
            # Create a span
            span = trace.span(name=interaction_type)
            if metadata:
                span.metadata = metadata
            if tags:
                span.tags = tags
            
            yield span
            
        except Exception as e:
            logger.error(f"Error in trace context: {e}")
            yield None

    def get_handler(self) -> CallbackHandler:
        """Get the Langfuse callback handler for LangChain"""
        return self.handler

    def add_score(self, 
                 name: str, 
                 value: float, 
                 comment: Optional[str] = None,
                 trace_id: Optional[str] = None) -> None:
        """Add a score to a trace"""
        try:
            if trace_id:
                self.langfuse.score(
                    trace_id=trace_id,
                    name=name,
                    value=value,
                    comment=comment
                )
            else:
                # Create a new trace for the score
                trace = self.langfuse.trace(
                    name=f"score_{name}",
                    metadata={"score_name": name}
                )
                span = trace.span(name="add_score")
                span.score(
                    name=name,
                    value=value,
                    comment=comment
                )
                    
            logger.debug(f"Added score: {name}={value}")
        except Exception as e:
            logger.error(f"Error adding score: {e}")

    def log_memory_access(self, thread_id: str, message_count: int, tags: Optional[List[str]] = None) -> None:
        """Log memory access"""
        try:
            trace = self.langfuse.trace(
                name="memory_access",
                metadata={
                    "thread_id": thread_id,
                    "message_count": message_count
                },
                tags=tags or ["memory", "history", "conversation"]
            )
            span = trace.span(name="memory_access")
            span.score(
                name="memory_access",
                value=1.0,
                comment=f"Retrieved {message_count} messages"
            )
                
        except Exception as e:
            logger.error(f"Error logging memory access: {e}")

    def flush(self):
        """Flush any pending traces"""
        try:
            self.langfuse.flush()
        except Exception as e:
            logger.error(f"Error flushing traces: {e}")

    def shutdown(self):
        """Shutdown the tracing service"""
        try:
            self.flush()
            self.langfuse.shutdown()
        except Exception as e:
            logger.error(f"Error shutting down tracing service: {e}")

tracing_service = TracingService() 
```
